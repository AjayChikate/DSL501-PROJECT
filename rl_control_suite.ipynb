{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7f604e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Environment: cheetah-run\n",
      "Obs dim: 17, Act dim: 6\n",
      "[  1000] Critic Loss1: 5.9753e-02 | Critic Loss2: 1.1533e+00 | Actor Loss: nan | KL: nan | Time: 0.1s\n",
      "[  1000] Updated reference policy\n",
      "[  2000] Critic Loss1: 3.5817e-02 | Critic Loss2: 3.9183e-02 | Actor Loss: nan | KL: nan | Time: 11.4s\n",
      "[  2000] Updated reference policy\n",
      "[  3000] Critic Loss1: 5.9890e-02 | Critic Loss2: 5.8972e-02 | Actor Loss: nan | KL: nan | Time: 24.4s\n",
      "[  3000] Updated reference policy\n",
      "[  4000] Critic Loss1: 7.4012e-02 | Critic Loss2: 7.5822e-02 | Actor Loss: nan | KL: nan | Time: 42.8s\n",
      "[  4000] Updated reference policy\n",
      "[  5000] Critic Loss1: 2.0764e-01 | Critic Loss2: 2.0971e-01 | Actor Loss: nan | KL: nan | Time: 55.1s\n",
      "[  5000] Updated reference policy\n",
      "[  5000] ===== EVAL: 0.40 =====\n",
      "[  6000] Critic Loss1: 2.2547e-01 | Critic Loss2: 2.4062e-01 | Actor Loss: -0.4481 | KL: 0.1163 | Time: 124.5s\n",
      "[  6000] Updated reference policy\n",
      "[  7000] Critic Loss1: 9.4497e-02 | Critic Loss2: 9.1470e-02 | Actor Loss: -0.4160 | KL: 0.1076 | Time: 146.7s\n",
      "[  7000] Updated reference policy\n",
      "[  8000] Critic Loss1: 8.4818e-02 | Critic Loss2: 8.2207e-02 | Actor Loss: -0.3484 | KL: 0.0911 | Time: 177.0s\n",
      "[  8000] Updated reference policy\n",
      "[  9000] Critic Loss1: 8.5348e-02 | Critic Loss2: 7.8932e-02 | Actor Loss: -0.3259 | KL: 0.0770 | Time: 200.1s\n",
      "[  9000] Updated reference policy\n",
      "[ 10000] Critic Loss1: 7.4197e-02 | Critic Loss2: 8.0297e-02 | Actor Loss: -0.2851 | KL: 0.0666 | Time: 225.3s\n",
      "[ 10000] Updated reference policy\n",
      "[ 10000] ===== EVAL: 0.46 =====\n",
      "[ 11000] Critic Loss1: 4.4230e-02 | Critic Loss2: 4.3650e-02 | Actor Loss: -0.2871 | KL: 0.0476 | Time: 260.5s\n",
      "[ 11000] Updated reference policy\n",
      "Step  11001 | Episode   10 | Reward:     5.31 | Length: 2000 | Time: 260.6s\n",
      "[ 12000] Critic Loss1: 3.4217e-02 | Critic Loss2: 3.4003e-02 | Actor Loss: -0.1907 | KL: 0.0510 | Time: 297.3s\n",
      "[ 12000] Updated reference policy\n",
      "[ 13000] Critic Loss1: 3.5377e-02 | Critic Loss2: 3.3461e-02 | Actor Loss: -0.0816 | KL: 0.0484 | Time: 334.0s\n",
      "[ 13000] Updated reference policy\n",
      "[ 14000] Critic Loss1: 1.0624e-02 | Critic Loss2: 9.3235e-03 | Actor Loss: -0.0298 | KL: 0.0415 | Time: 356.8s\n",
      "[ 14000] Updated reference policy\n",
      "[ 15000] Critic Loss1: 8.0901e-03 | Critic Loss2: 9.0998e-03 | Actor Loss: -0.2027 | KL: 0.0378 | Time: 388.2s\n",
      "[ 15000] Updated reference policy\n",
      "[ 15000] ===== EVAL: 0.60 =====\n",
      "[ 16000] Critic Loss1: 9.1833e-03 | Critic Loss2: 9.4638e-03 | Actor Loss: -0.0346 | KL: 0.0298 | Time: 434.4s\n",
      "[ 16000] Updated reference policy\n",
      "[ 17000] Critic Loss1: 5.1408e-03 | Critic Loss2: 5.3538e-03 | Actor Loss: -0.1344 | KL: 0.0275 | Time: 456.0s\n",
      "[ 17000] Updated reference policy\n",
      "[ 18000] Critic Loss1: 4.6713e-03 | Critic Loss2: 4.4341e-03 | Actor Loss: -0.1743 | KL: 0.0213 | Time: 492.6s\n",
      "[ 18000] Updated reference policy\n",
      "[ 19000] Critic Loss1: 4.6426e-03 | Critic Loss2: 5.3319e-03 | Actor Loss: -0.1523 | KL: 0.0204 | Time: 528.1s\n",
      "[ 19000] Updated reference policy\n",
      "[ 20000] Critic Loss1: 2.1762e-03 | Critic Loss2: 2.3852e-03 | Actor Loss: -0.1178 | KL: 0.0180 | Time: 556.3s\n",
      "[ 20000] Updated reference policy\n",
      "[ 20000] ===== EVAL: 0.44 =====\n",
      "[ 21000] Critic Loss1: 3.2150e-03 | Critic Loss2: 3.9016e-03 | Actor Loss: -0.1678 | KL: 0.0212 | Time: 585.5s\n",
      "[ 21000] Updated reference policy\n",
      "[ 22000] Critic Loss1: 2.1708e-03 | Critic Loss2: 2.0756e-03 | Actor Loss: -0.0526 | KL: 0.0264 | Time: 610.4s\n",
      "[ 22000] Updated reference policy\n",
      "[ 23000] Critic Loss1: 3.1453e-03 | Critic Loss2: 3.3416e-03 | Actor Loss: -0.0566 | KL: 0.0276 | Time: 632.6s\n",
      "[ 23000] Updated reference policy\n",
      "Step  23001 | Episode   20 | Reward:     2.12 | Length: 1000 | Time: 632.6s\n",
      "[ 24000] Critic Loss1: 1.6594e-03 | Critic Loss2: 1.5449e-03 | Actor Loss: -0.1500 | KL: 0.0189 | Time: 655.4s\n",
      "[ 24000] Updated reference policy\n",
      "[ 25000] Critic Loss1: 2.2036e-03 | Critic Loss2: 1.9142e-03 | Actor Loss: -0.0386 | KL: 0.0204 | Time: 680.4s\n",
      "[ 25000] Updated reference policy\n",
      "[ 25000] ===== EVAL: 0.53 =====\n",
      "[ 26000] Critic Loss1: 2.5243e-03 | Critic Loss2: 2.5065e-03 | Actor Loss: -0.1384 | KL: 0.0207 | Time: 706.3s\n",
      "[ 26000] Updated reference policy\n",
      "[ 27000] Critic Loss1: 1.5839e-03 | Critic Loss2: 1.6315e-03 | Actor Loss: -0.0751 | KL: 0.0213 | Time: 728.4s\n",
      "[ 27000] Updated reference policy\n",
      "[ 28000] Critic Loss1: 2.1955e-03 | Critic Loss2: 2.2547e-03 | Actor Loss: -0.1433 | KL: 0.0176 | Time: 755.0s\n",
      "[ 28000] Updated reference policy\n",
      "[ 29000] Critic Loss1: 1.6518e-03 | Critic Loss2: 1.6064e-03 | Actor Loss: -0.0572 | KL: 0.0216 | Time: 796.5s\n",
      "[ 29000] Updated reference policy\n",
      "[ 30000] Critic Loss1: 1.5832e-03 | Critic Loss2: 1.6394e-03 | Actor Loss: -0.2051 | KL: 0.0168 | Time: 815.4s\n",
      "[ 30000] Updated reference policy\n",
      "[ 30000] ===== EVAL: 0.59 =====\n",
      "[ 31000] Critic Loss1: 9.3391e-04 | Critic Loss2: 9.2308e-04 | Actor Loss: -0.0742 | KL: 0.0226 | Time: 835.2s\n",
      "[ 31000] Updated reference policy\n",
      "[ 32000] Critic Loss1: 1.4428e-03 | Critic Loss2: 1.4339e-03 | Actor Loss: 0.0033 | KL: 0.0240 | Time: 851.6s\n",
      "[ 32000] Updated reference policy\n",
      "[ 33000] Critic Loss1: 8.6828e-04 | Critic Loss2: 9.0167e-04 | Actor Loss: -0.1712 | KL: 0.0165 | Time: 868.4s\n",
      "[ 33000] Updated reference policy\n",
      "[ 34000] Critic Loss1: 1.5077e-03 | Critic Loss2: 1.5495e-03 | Actor Loss: -0.0614 | KL: 0.0165 | Time: 885.3s\n",
      "[ 34000] Updated reference policy\n",
      "[ 35000] Critic Loss1: 2.1426e-03 | Critic Loss2: 1.9224e-03 | Actor Loss: -0.0462 | KL: 0.0208 | Time: 901.4s\n",
      "[ 35000] Updated reference policy\n",
      "[ 35000] ===== EVAL: 1.34 =====\n",
      "[ 36000] Critic Loss1: 9.9448e-04 | Critic Loss2: 9.3656e-04 | Actor Loss: -0.1343 | KL: 0.0179 | Time: 931.1s\n",
      "[ 36000] Updated reference policy\n",
      "Step  36001 | Episode   30 | Reward:     5.48 | Length: 2000 | Time: 931.2s\n",
      "[ 37000] Critic Loss1: 1.1537e-03 | Critic Loss2: 1.0771e-03 | Actor Loss: -0.0337 | KL: 0.0238 | Time: 953.5s\n",
      "[ 37000] Updated reference policy\n",
      "[ 38000] Critic Loss1: 7.4834e-04 | Critic Loss2: 6.4477e-04 | Actor Loss: -0.0813 | KL: 0.0159 | Time: 978.7s\n",
      "[ 38000] Updated reference policy\n",
      "[ 39000] Critic Loss1: 5.7868e-04 | Critic Loss2: 4.3786e-04 | Actor Loss: -0.0655 | KL: 0.0173 | Time: 1001.9s\n",
      "[ 39000] Updated reference policy\n",
      "[ 40000] Critic Loss1: 5.8506e-04 | Critic Loss2: 5.1967e-04 | Actor Loss: -0.0392 | KL: 0.0166 | Time: 1019.4s\n",
      "[ 40000] Updated reference policy\n",
      "[ 40000] ===== EVAL: 12.23 =====\n",
      "[ 41000] Critic Loss1: 1.2099e-03 | Critic Loss2: 1.3084e-03 | Actor Loss: -0.1185 | KL: 0.0175 | Time: 1037.3s\n",
      "[ 41000] Updated reference policy\n",
      "[ 42000] Critic Loss1: 1.3822e-03 | Critic Loss2: 1.4789e-03 | Actor Loss: -0.1327 | KL: 0.0286 | Time: 1055.0s\n",
      "[ 42000] Updated reference policy\n",
      "[ 43000] Critic Loss1: 7.8743e-04 | Critic Loss2: 6.9270e-04 | Actor Loss: -0.1017 | KL: 0.0171 | Time: 1076.4s\n",
      "[ 43000] Updated reference policy\n",
      "[ 44000] Critic Loss1: 4.5594e-04 | Critic Loss2: 3.9312e-04 | Actor Loss: -0.0476 | KL: 0.0191 | Time: 1095.7s\n",
      "[ 44000] Updated reference policy\n",
      "[ 45000] Critic Loss1: 8.1579e-04 | Critic Loss2: 7.2560e-04 | Actor Loss: -0.0346 | KL: 0.0284 | Time: 1115.6s\n",
      "[ 45000] Updated reference policy\n",
      "[ 45000] ===== EVAL: 20.10 =====\n",
      "[ 46000] Critic Loss1: 7.9476e-04 | Critic Loss2: 8.1264e-04 | Actor Loss: -0.0524 | KL: 0.0318 | Time: 1141.6s\n",
      "[ 46000] Updated reference policy\n",
      "[ 47000] Critic Loss1: 8.7239e-04 | Critic Loss2: 7.2450e-04 | Actor Loss: -0.0415 | KL: 0.0276 | Time: 1165.1s\n",
      "[ 47000] Updated reference policy\n",
      "[ 48000] Critic Loss1: 1.1720e-03 | Critic Loss2: 1.1338e-03 | Actor Loss: -0.0697 | KL: 0.0314 | Time: 1183.6s\n",
      "[ 48000] Updated reference policy\n",
      "Step  48001 | Episode   40 | Reward:    23.60 | Length: 1000 | Time: 1183.6s\n",
      "[ 49000] Critic Loss1: 6.0045e-04 | Critic Loss2: 6.6474e-04 | Actor Loss: -0.1036 | KL: 0.0424 | Time: 1201.7s\n",
      "[ 49000] Updated reference policy\n",
      "[ 50000] Critic Loss1: 1.0331e-03 | Critic Loss2: 8.4327e-04 | Actor Loss: -0.0581 | KL: 0.0322 | Time: 1223.7s\n",
      "[ 50000] Updated reference policy\n",
      "[ 50000] ===== EVAL: 31.88 =====\n"
     ]
    }
   ],
   "source": [
    "# pmpo_fixed.py\n",
    "# PMPO - stabilized implementation (PyTorch + dm_control)\n",
    "# Changes:\n",
    "# - Twin critics (Double Q) with min-target\n",
    "# - Hard target sync every target_update_freq steps\n",
    "# - Reduced critic LR and actor LR defaults\n",
    "# - Critic warm-up before actor updates\n",
    "# - Clip TD targets and Q predictions\n",
    "# - Observation normalization\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dm_control import suite\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ---------------------- Utilities ----------------------\n",
    "class RunningMeanStd:\n",
    "    \"\"\"Online running mean/std for observation normalization.\"\"\"\n",
    "    def __init__(self, shape, eps=1e-4, clip=10.0):\n",
    "        self.mean = np.zeros(shape, dtype=np.float64)\n",
    "        self.var = np.ones(shape, dtype=np.float64)\n",
    "        self.count = eps\n",
    "        self.clip = clip\n",
    "\n",
    "    def update(self, x: np.ndarray):\n",
    "        # x is (N, dim) or (dim,)\n",
    "        x = np.asarray(x)\n",
    "        if x.ndim == 1:\n",
    "            x = x[None, :]\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_var = np.var(x, axis=0)\n",
    "        batch_count = x.shape[0]\n",
    "        self._update_from_moments(batch_mean, batch_var, batch_count)\n",
    "\n",
    "    def _update_from_moments(self, batch_mean, batch_var, batch_count):\n",
    "        # Welford-like update\n",
    "        delta = batch_mean - self.mean\n",
    "        tot_count = self.count + batch_count\n",
    "\n",
    "        new_mean = self.mean + delta * batch_count / tot_count\n",
    "        m_a = self.var * (self.count)\n",
    "        m_b = batch_var * (batch_count)\n",
    "        M2 = m_a + m_b + (delta ** 2) * self.count * batch_count / tot_count\n",
    "        new_var = M2 / (tot_count)\n",
    "\n",
    "        self.mean = new_mean\n",
    "        self.var = new_var\n",
    "        self.count = tot_count\n",
    "\n",
    "    def normalize(self, x: np.ndarray):\n",
    "        return np.clip((x - self.mean) / (np.sqrt(self.var) + 1e-8), -self.clip, self.clip)\n",
    "\n",
    "def flatten_obs(obs):\n",
    "    \"\"\"Flatten observation dict to single array.\"\"\"\n",
    "    if isinstance(obs, dict):\n",
    "        parts = [np.asarray(v).ravel() for v in obs.values()]\n",
    "        return np.concatenate(parts).astype(np.float32)\n",
    "    else:\n",
    "        return np.asarray(obs).astype(np.float32).ravel()\n",
    "\n",
    "# ---------------------- Replay Buffer ----------------------\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Efficient replay buffer with numpy storage.\"\"\"\n",
    "    def __init__(self, size=200000):\n",
    "        self.max_size = size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        \n",
    "        # Pre-allocate arrays (filled on first push)\n",
    "        self.states = None\n",
    "        self.actions = None\n",
    "        self.rewards = None\n",
    "        self.next_states = None\n",
    "        self.dones = None\n",
    "\n",
    "    def push(self, s, a, r, s2, d):\n",
    "        \"\"\"Store transition.\"\"\"\n",
    "        # Initialize arrays on first push\n",
    "        if self.states is None:\n",
    "            self.states = np.zeros((self.max_size, len(s)), dtype=np.float32)\n",
    "            self.actions = np.zeros((self.max_size, len(a)), dtype=np.float32)\n",
    "            self.rewards = np.zeros(self.max_size, dtype=np.float32)\n",
    "            self.next_states = np.zeros((self.max_size, len(s2)), dtype=np.float32)\n",
    "            self.dones = np.zeros(self.max_size, dtype=np.float32)\n",
    "        \n",
    "        self.states[self.ptr] = s\n",
    "        self.actions[self.ptr] = a\n",
    "        self.rewards[self.ptr] = r\n",
    "        self.next_states[self.ptr] = s2\n",
    "        self.dones[self.ptr] = d\n",
    "        \n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample batch and convert to GPU tensors once.\"\"\"\n",
    "        idx = np.random.randint(0, self.size, size=batch_size)\n",
    "        \n",
    "        return (\n",
    "            torch.from_numpy(self.states[idx]).to(device),\n",
    "            torch.from_numpy(self.actions[idx]).to(device),\n",
    "            torch.from_numpy(self.rewards[idx]).to(device).unsqueeze(-1),\n",
    "            torch.from_numpy(self.next_states[idx]).to(device),\n",
    "            torch.from_numpy(self.dones[idx]).to(device).unsqueeze(-1)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "# ---------------------- Networks ----------------------\n",
    "def orthogonal_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "        nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Gaussian policy network.\"\"\"\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=256, log_std_min=-5, log_std_max=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.mu = nn.Linear(hidden_dim, act_dim)\n",
    "        self.log_std = nn.Linear(hidden_dim, act_dim)\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        self.apply(orthogonal_init)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        h = self.net(obs)\n",
    "        mu = self.mu(h)\n",
    "        log_std = torch.tanh(self.log_std(h))  # bound to (-1,1)\n",
    "        # scale to range\n",
    "        log_std = self.log_std_min + 0.5 * (log_std + 1.0) * (self.log_std_max - self.log_std_min)\n",
    "        std = torch.exp(log_std)\n",
    "        return mu, std, log_std\n",
    "\n",
    "    def sample(self, obs):\n",
    "        mu, std, log_std = self.forward(obs)\n",
    "        eps = torch.randn_like(mu)\n",
    "        a = mu + eps * std\n",
    "        var = std ** 2\n",
    "        log_prob = -0.5 * (((a - mu) ** 2) / var + 2 * log_std + math.log(2 * math.pi))\n",
    "        log_prob = log_prob.sum(-1, keepdim=True)\n",
    "        return a, log_prob\n",
    "\n",
    "    def log_prob(self, obs, a):\n",
    "        mu, std, log_std = self.forward(obs)\n",
    "        var = std ** 2\n",
    "        log_prob = -0.5 * (((a - mu) ** 2) / var + 2 * log_std + math.log(2 * math.pi))\n",
    "        return log_prob.sum(-1, keepdim=True)\n",
    "\n",
    "    def kl(self, obs, other_mu, other_std):\n",
    "        mu, std, _ = self.forward(obs)\n",
    "        var = std ** 2\n",
    "        other_var = other_std ** 2\n",
    "        kl_div = (torch.log(std / other_std) + (other_var + (other_mu - mu) ** 2) / (2 * var) - 0.5)\n",
    "        return kl_div.sum(-1, keepdim=True)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Q-function network.\"\"\"\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim, hidden_dim), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        self.apply(orthogonal_init)\n",
    "    \n",
    "    def forward(self, s, a):\n",
    "        return self.net(torch.cat([s, a], dim=-1))\n",
    "\n",
    "# ---------------------- PMPO Trainer (stabilized) ----------------------\n",
    "class PMPO:\n",
    "    \"\"\"Preference Matching Policy Optimization with twin critics and stability fixes.\"\"\"\n",
    "    def __init__(self, obs_dim, act_dim, \n",
    "                 actor_lr=1e-5, critic_lr=1e-4,\n",
    "                 gamma=0.99, tau=0.995,\n",
    "                 alpha=0.5, beta=1.0,\n",
    "                 grad_clip=1.0,\n",
    "                 target_update_freq=250,\n",
    "                 critic_warmup_steps=5000,\n",
    "                 actor_update_every=2,\n",
    "                 q_clip=1e6):\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau  # used only for soft updates if desired\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.grad_clip = grad_clip\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.critic_warmup_steps = critic_warmup_steps\n",
    "        self.actor_update_every = actor_update_every\n",
    "        self.q_clip = q_clip\n",
    "\n",
    "        # Networks: actor + reference actor\n",
    "        self.actor = Actor(obs_dim, act_dim).to(device)\n",
    "        self.ref_actor = Actor(obs_dim, act_dim).to(device)\n",
    "        self.ref_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.ref_actor.eval()\n",
    "\n",
    "        # Twin critics (Q1, Q2) and twin targets\n",
    "        self.critic1 = Critic(obs_dim, act_dim).to(device)\n",
    "        self.critic2 = Critic(obs_dim, act_dim).to(device)\n",
    "        self.target_critic1 = Critic(obs_dim, act_dim).to(device)\n",
    "        self.target_critic2 = Critic(obs_dim, act_dim).to(device)\n",
    "        self.target_critic1.load_state_dict(self.critic1.state_dict())\n",
    "        self.target_critic2.load_state_dict(self.critic2.state_dict())\n",
    "\n",
    "        # Optimizers\n",
    "        self.actor_opt = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic1_opt = torch.optim.Adam(self.critic1.parameters(), lr=critic_lr)\n",
    "        self.critic2_opt = torch.optim.Adam(self.critic2.parameters(), lr=critic_lr)\n",
    "\n",
    "        # Counters\n",
    "        self._total_steps = 0\n",
    "        self._critic_updates = 0\n",
    "\n",
    "    def update_critic(self, batch):\n",
    "        \"\"\"Update twin critics with min-target to avoid overestimation.\"\"\"\n",
    "        s, a, r, s2, d = batch\n",
    "\n",
    "        # clamp rewards moderately (optional) to keep scale manageable\n",
    "        r = torch.clamp(r, -1e3, 1e3)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # next actions from current actor\n",
    "            a2, _ = self.actor.sample(s2)\n",
    "            q1_next = self.target_critic1(s2, a2)\n",
    "            q2_next = self.target_critic2(s2, a2)\n",
    "            q_next = torch.min(q1_next, q2_next)\n",
    "\n",
    "            y = r + self.gamma * (1.0 - d) * q_next\n",
    "            y = torch.clamp(y, -self.q_clip, self.q_clip)\n",
    "\n",
    "        # Critic 1\n",
    "        q1 = self.critic1(s, a)\n",
    "        loss1 = F.mse_loss(q1, y)\n",
    "        self.critic1_opt.zero_grad()\n",
    "        loss1.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic1.parameters(), self.grad_clip)\n",
    "        self.critic1_opt.step()\n",
    "\n",
    "        # Critic 2\n",
    "        q2 = self.critic2(s, a)\n",
    "        loss2 = F.mse_loss(q2, y)\n",
    "        self.critic2_opt.zero_grad()\n",
    "        loss2.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic2.parameters(), self.grad_clip)\n",
    "        self.critic2_opt.step()\n",
    "\n",
    "        # Hard target sync periodically (more stable than immediate polyak if tuned)\n",
    "        self._critic_updates += 1\n",
    "        if (self._critic_updates % self.target_update_freq) == 0:\n",
    "            self.target_critic1.load_state_dict(self.critic1.state_dict())\n",
    "            self.target_critic2.load_state_dict(self.critic2.state_dict())\n",
    "\n",
    "        return (loss1.item(), loss2.item())\n",
    "\n",
    "    def update_actor(self, s, K=16):\n",
    "        \"\"\"Update policy with preference-based objective (PMPO).\"\"\"\n",
    "        B = s.size(0)\n",
    "\n",
    "        # Sample K actions per state from reference policy (stay on device)\n",
    "        s_rep = s[:, None, :].expand(B, K, -1).reshape(B * K, -1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            mu_ref, std_ref, _ = self.ref_actor.forward(s_rep)\n",
    "            eps = torch.randn_like(mu_ref)\n",
    "            acts = mu_ref + eps * std_ref\n",
    "            qs = self.critic1(s_rep, acts).reshape(B, K)  # could use avg of critics, using critic1 for ranking is fine\n",
    "\n",
    "        # Rank actions\n",
    "        idx = torch.argsort(qs, dim=1)  # ascending\n",
    "        top2_idx = idx[:, -2:]  # highest Q-values\n",
    "        bot2_idx = idx[:, :2]   # lowest Q-values\n",
    "\n",
    "        # Advanced indexing to gather actions/states (no Python loops)\n",
    "        batch_idx = torch.arange(B, device=device)[:, None].expand(B, 2)\n",
    "        top_flat = (batch_idx * K + top2_idx).reshape(-1)\n",
    "        bot_flat = (batch_idx * K + bot2_idx).reshape(-1)\n",
    "\n",
    "        pref_s = s.repeat_interleave(2, dim=0)\n",
    "        pref_a = acts[top_flat]\n",
    "        rej_s = s.repeat_interleave(2, dim=0)\n",
    "        rej_a = acts[bot_flat]\n",
    "\n",
    "        # Compute PMPO terms\n",
    "        logp_pref = self.actor.log_prob(pref_s, pref_a).mean()\n",
    "        logp_rej = self.actor.log_prob(rej_s, rej_a).mean()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mu_r, std_r, _ = self.ref_actor.forward(s)\n",
    "        kl = self.actor.kl(s, mu_r, std_r).mean()\n",
    "\n",
    "        # PMPO objective: maximize J -> minimize -J\n",
    "        J = self.alpha * logp_pref - (1.0 - self.alpha) * logp_rej - self.beta * kl\n",
    "        loss = -J\n",
    "\n",
    "        self.actor_opt.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.grad_clip)\n",
    "        self.actor_opt.step()\n",
    "\n",
    "        return loss.item(), kl.item()\n",
    "\n",
    "    def update_reference(self):\n",
    "        \"\"\"Update reference policy to current policy.\"\"\"\n",
    "        self.ref_actor.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, obs, deterministic=False):\n",
    "        \"\"\"Select action (for evaluation).\"\"\"\n",
    "        obs_t = torch.from_numpy(obs).float().to(device).unsqueeze(0)\n",
    "        if deterministic:\n",
    "            mu, _, _ = self.actor.forward(obs_t)\n",
    "            return mu.cpu().numpy()[0]\n",
    "        else:\n",
    "            a, _ = self.actor.sample(obs_t)\n",
    "            return a.cpu().numpy()[0]\n",
    "\n",
    "# ---------------------- Training Loop ----------------------\n",
    "def train(domain=\"cheetah\", task=\"run\", \n",
    "          total_steps=200000,\n",
    "          batch_size=256,\n",
    "          start_steps=1000,\n",
    "          ref_update_freq=1000,\n",
    "          eval_freq=5000,\n",
    "          eval_episodes=5,\n",
    "          seed=0):\n",
    "    \"\"\"\n",
    "    Main training loop with stabilization changes.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    env = suite.load(domain, task)\n",
    "    ts = env.reset()\n",
    "    s = flatten_obs(ts.observation)\n",
    "\n",
    "    # obs normalization\n",
    "    obs_rms = RunningMeanStd(len(s))\n",
    "\n",
    "    obs_dim = s.size\n",
    "    act_dim = env.action_spec().shape[0]\n",
    "    \n",
    "    print(f\"Environment: {domain}-{task}\")\n",
    "    print(f\"Obs dim: {obs_dim}, Act dim: {act_dim}\")\n",
    "    \n",
    "    # Initialize\n",
    "    rb = ReplayBuffer(size=200000)\n",
    "    algo = PMPO(obs_dim, act_dim,\n",
    "                actor_lr=1e-5,    # smaller actor lr\n",
    "                critic_lr=1e-4,   # smaller critic lr\n",
    "                gamma=0.99,\n",
    "                tau=0.995,\n",
    "                alpha=0.5,\n",
    "                beta=1.0,\n",
    "                grad_clip=1.0,\n",
    "                target_update_freq=250,\n",
    "                critic_warmup_steps=5000,\n",
    "                actor_update_every=2,\n",
    "                q_clip=1e5)\n",
    "\n",
    "    # Action bounds\n",
    "    act_low = env.action_spec().minimum\n",
    "    act_high = env.action_spec().maximum\n",
    "    \n",
    "    # Metrics\n",
    "    episode_reward = 0.0\n",
    "    episode_step = 0\n",
    "    episode_count = 0\n",
    "    start_time = time.time()\n",
    "    last_eval = 0\n",
    "\n",
    "    for t in range(1, total_steps + 1):\n",
    "        # Select action\n",
    "        if len(rb) < start_steps:\n",
    "            a = np.random.uniform(act_low, act_high, size=act_dim).astype(np.float32)\n",
    "        else:\n",
    "            # normalize obs\n",
    "            obs_norm = obs_rms.normalize(s)\n",
    "            a = algo.act(obs_norm, deterministic=False)\n",
    "            a = np.clip(a, act_low, act_high)\n",
    "\n",
    "        # Environment step\n",
    "        ts = env.step(a)\n",
    "        s2 = flatten_obs(ts.observation)\n",
    "        k = ts.reward\n",
    "        if k is None:\n",
    "            k = 0.0\n",
    "        r = float(k)\n",
    "        d = float(ts.last())\n",
    "        \n",
    "        episode_reward += r\n",
    "        episode_step += 1\n",
    "        \n",
    "        # Store transition (store raw obs, normalize when sampling/training)\n",
    "        rb.push(s, a, r, s2, d)\n",
    "\n",
    "        # Update running obs stats (do it on CPU numpy)\n",
    "        obs_rms.update(s2)\n",
    "\n",
    "        # Reset handling\n",
    "        if d:\n",
    "            episode_count += 1\n",
    "            if episode_count % 10 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"Step {t:6d} | Episode {episode_count:4d} | \"\n",
    "                      f\"Reward: {episode_reward:8.2f} | Length: {episode_step:4d} | Time: {elapsed:.1f}s\")\n",
    "            \n",
    "            ts = env.reset()\n",
    "            s = flatten_obs(ts.observation)\n",
    "            episode_reward = 0.0\n",
    "            episode_step = 0\n",
    "        else:\n",
    "            s = s2\n",
    "\n",
    "        # Training updates\n",
    "        if len(rb) >= start_steps:\n",
    "            batch = rb.sample(batch_size)\n",
    "\n",
    "            # BEFORE feeding to networks, normalize states and next_states in-tensor\n",
    "            bs, ba, br, bs2, bd = batch\n",
    "            # Convert to numpy to normalize with obs_rms then back (we avoid CPU-GPU ping-pong by normalizing on CPU once per batch)\n",
    "            bs_np = bs.cpu().numpy()\n",
    "            bs2_np = bs2.cpu().numpy()\n",
    "            bs_norm = torch.from_numpy(np.clip((bs_np - obs_rms.mean) / (np.sqrt(obs_rms.var) + 1e-8), -obs_rms.clip, obs_rms.clip)).float().to(device)\n",
    "            bs2_norm = torch.from_numpy(np.clip((bs2_np - obs_rms.mean) / (np.sqrt(obs_rms.var) + 1e-8), -obs_rms.clip, obs_rms.clip)).float().to(device)\n",
    "\n",
    "            # Replace states in batch with normalized tensors for training\n",
    "            train_batch = (bs_norm, ba, br, bs2_norm, bd)\n",
    "\n",
    "            # Update critics (twin)\n",
    "            loss1, loss2 = algo.update_critic(train_batch)\n",
    "            critic_loss = (loss1 + loss2) / 2.0\n",
    "\n",
    "            # Actor update schedule & warmup\n",
    "            if (t > algo.critic_warmup_steps) and (t % algo.actor_update_every == 0):\n",
    "                # pass normalized states\n",
    "                actor_loss, kl = algo.update_actor(bs_norm, K=16)\n",
    "            else:\n",
    "                actor_loss, kl = float('nan'), float('nan')\n",
    "\n",
    "            # Periodic status print\n",
    "            if t % 1000 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"[{t:6d}] Critic Loss1: {loss1:.4e} | Critic Loss2: {loss2:.4e} | \"\n",
    "                      f\"Actor Loss: {actor_loss:.4f} | KL: {kl:.4f} | Time: {elapsed:.1f}s\")\n",
    "\n",
    "        # Update reference policy\n",
    "        if t % ref_update_freq == 0 and t > 0:\n",
    "            algo.update_reference()\n",
    "            print(f\"[{t:6d}] Updated reference policy\")\n",
    "\n",
    "        # Evaluation\n",
    "        if t % eval_freq == 0 and t > 0 and (t - last_eval) >= eval_freq:\n",
    "            eval_reward = evaluate(env, algo, eval_episodes, act_low, act_high, obs_rms)\n",
    "            print(f\"[{t:6d}] ===== EVAL: {eval_reward:.2f} =====\")\n",
    "            last_eval = t\n",
    "\n",
    "    print(\"\\nTraining finished!\")\n",
    "    return algo\n",
    "\n",
    "def evaluate(env, algo, num_episodes, act_low, act_high, obs_rms):\n",
    "    \"\"\"Evaluate policy deterministically (uses obs normalization).\"\"\"\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        ts = env.reset()\n",
    "        s = flatten_obs(ts.observation)\n",
    "        ep_reward = 0.0\n",
    "        \n",
    "        while not ts.last():\n",
    "            s_norm = obs_rms.normalize(s)\n",
    "            a = algo.act(s_norm, deterministic=True)\n",
    "            a = np.clip(a, act_low, act_high)\n",
    "            ts = env.step(a)\n",
    "            s = flatten_obs(ts.observation)\n",
    "            r = ts.reward\n",
    "            if r is None:\n",
    "                r = 0.0\n",
    "            ep_reward += r\n",
    "        \n",
    "        total_reward += ep_reward\n",
    "    \n",
    "    return total_reward / num_episodes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_agent = train(\n",
    "        domain=\"cheetah\",\n",
    "        task=\"run\",\n",
    "        total_steps=200000,\n",
    "        batch_size=256,\n",
    "        start_steps=1000,\n",
    "        ref_update_freq=1000,\n",
    "        eval_freq=5000,\n",
    "        eval_episodes=5,\n",
    "        seed=0\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
