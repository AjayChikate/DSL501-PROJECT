{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c37d130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naman/.venv/lib/python3.10/site-packages/glfw/__init__.py:917: GLFWError: (65550) b'X11: The DISPLAY environment variable is missing'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Environment: cartpole-swingup\n",
      "Obs dim: 5, Act dim: 1\n",
      "[  1000] Critic Loss1: 9.4578e-03 | Critic Loss2: 1.5406e-01 | Actor Loss: nan | KL: nan | Time: 4.5s\n",
      "[  1000] Updated reference policy\n",
      "[  2000] Critic Loss1: 4.0722e-02 | Critic Loss2: 4.0519e-02 | Actor Loss: nan | KL: nan | Time: 43.4s\n",
      "[  2000] Updated reference policy\n",
      "[  3000] Critic Loss1: 2.3887e-02 | Critic Loss2: 2.3661e-02 | Actor Loss: nan | KL: nan | Time: 81.9s\n",
      "[  3000] Updated reference policy\n",
      "[  4000] Critic Loss1: 1.8952e-02 | Critic Loss2: 1.8933e-02 | Actor Loss: nan | KL: nan | Time: 120.4s\n",
      "[  4000] Updated reference policy\n",
      "[  5000] Critic Loss1: 1.2607e-02 | Critic Loss2: 1.2739e-02 | Actor Loss: nan | KL: nan | Time: 158.9s\n",
      "[  5000] Updated reference policy\n",
      "[  5000] ===== EVAL: 12.25 =====\n",
      "[  6000] Critic Loss1: 1.0848e-02 | Critic Loss2: 1.1305e-02 | Actor Loss: -0.0910 | KL: 0.9486 | Time: 237.8s\n",
      "[  6000] Updated reference policy\n",
      "[  7000] Critic Loss1: 1.4104e-02 | Critic Loss2: 1.4927e-02 | Actor Loss: -0.3051 | KL: 0.2847 | Time: 295.2s\n",
      "[  7000] Updated reference policy\n",
      "[  8000] Critic Loss1: 1.7503e-02 | Critic Loss2: 1.7426e-02 | Actor Loss: -0.1323 | KL: 0.2487 | Time: 352.5s\n",
      "[  8000] Updated reference policy\n",
      "[  9000] Critic Loss1: 1.9922e-02 | Critic Loss2: 1.9022e-02 | Actor Loss: -0.1934 | KL: 0.1958 | Time: 409.7s\n",
      "[  9000] Updated reference policy\n",
      "[ 10000] Critic Loss1: 2.0082e-02 | Critic Loss2: 1.9030e-02 | Actor Loss: -0.2067 | KL: 0.2343 | Time: 467.2s\n",
      "[ 10000] Updated reference policy\n",
      "[ 10000] ===== EVAL: 138.91 =====\n",
      "[ 11000] Critic Loss1: 1.2517e-02 | Critic Loss2: 1.2313e-02 | Actor Loss: -0.5326 | KL: 0.2406 | Time: 545.3s\n",
      "[ 11000] Updated reference policy\n",
      "Step  11001 | Episode   10 | Reward:   216.37 | Length: 2000 | Time: 545.3s\n",
      "[ 12000] Critic Loss1: 1.6772e-02 | Critic Loss2: 1.5847e-02 | Actor Loss: -0.3966 | KL: 0.2527 | Time: 602.7s\n",
      "[ 12000] Updated reference policy\n",
      "[ 13000] Critic Loss1: 1.7994e-01 | Critic Loss2: 1.7775e-01 | Actor Loss: -0.4454 | KL: 0.2549 | Time: 660.0s\n",
      "[ 13000] Updated reference policy\n",
      "[ 14000] Critic Loss1: 1.5385e-02 | Critic Loss2: 1.7693e-02 | Actor Loss: -0.2756 | KL: 0.2364 | Time: 717.4s\n",
      "[ 14000] Updated reference policy\n",
      "[ 15000] Critic Loss1: 3.0378e-02 | Critic Loss2: 2.9466e-02 | Actor Loss: -0.3694 | KL: 0.2813 | Time: 774.8s\n",
      "[ 15000] Updated reference policy\n",
      "[ 15000] ===== EVAL: 177.07 =====\n",
      "[ 16000] Critic Loss1: 2.5362e-02 | Critic Loss2: 2.3377e-02 | Actor Loss: -0.3665 | KL: 0.3010 | Time: 853.2s\n",
      "[ 16000] Updated reference policy\n",
      "[ 17000] Critic Loss1: 1.8150e-02 | Critic Loss2: 1.7332e-02 | Actor Loss: -0.4152 | KL: 0.3033 | Time: 910.7s\n",
      "[ 17000] Updated reference policy\n",
      "[ 18000] Critic Loss1: 2.0277e-02 | Critic Loss2: 2.1871e-02 | Actor Loss: -0.3429 | KL: 0.2783 | Time: 968.1s\n",
      "[ 18000] Updated reference policy\n",
      "[ 19000] Critic Loss1: 2.5532e-02 | Critic Loss2: 2.5495e-02 | Actor Loss: -0.4051 | KL: 0.2364 | Time: 1025.6s\n",
      "[ 19000] Updated reference policy\n",
      "[ 20000] Critic Loss1: 3.0836e-02 | Critic Loss2: 3.2237e-02 | Actor Loss: -0.4531 | KL: 0.2474 | Time: 1083.0s\n",
      "[ 20000] Updated reference policy\n",
      "[ 20000] ===== EVAL: 227.64 =====\n",
      "[ 21000] Critic Loss1: 9.8485e-01 | Critic Loss2: 1.0162e+00 | Actor Loss: -0.3774 | KL: 0.2524 | Time: 1161.4s\n",
      "[ 21000] Updated reference policy\n",
      "[ 22000] Critic Loss1: 2.3425e-02 | Critic Loss2: 2.8459e-02 | Actor Loss: -0.3944 | KL: 0.2430 | Time: 1218.9s\n",
      "[ 22000] Updated reference policy\n",
      "[ 23000] Critic Loss1: 2.1431e-02 | Critic Loss2: 1.9312e-02 | Actor Loss: -0.3407 | KL: 0.2700 | Time: 1276.4s\n",
      "[ 23000] Updated reference policy\n",
      "Step  23001 | Episode   20 | Reward:   191.71 | Length: 1000 | Time: 1276.4s\n",
      "[ 24000] Critic Loss1: 4.3601e-02 | Critic Loss2: 3.4743e-02 | Actor Loss: -0.3764 | KL: 0.2945 | Time: 1333.7s\n",
      "[ 24000] Updated reference policy\n",
      "[ 25000] Critic Loss1: 1.8968e-02 | Critic Loss2: 1.7057e-02 | Actor Loss: -0.4191 | KL: 0.2926 | Time: 1391.0s\n",
      "[ 25000] Updated reference policy\n",
      "[ 25000] ===== EVAL: 200.26 =====\n",
      "[ 26000] Critic Loss1: 1.9972e-02 | Critic Loss2: 1.9400e-02 | Actor Loss: -0.5340 | KL: 0.3120 | Time: 1469.4s\n",
      "[ 26000] Updated reference policy\n",
      "[ 27000] Critic Loss1: 4.1619e-01 | Critic Loss2: 4.0569e-01 | Actor Loss: -0.4958 | KL: 0.3015 | Time: 1527.2s\n",
      "[ 27000] Updated reference policy\n",
      "[ 28000] Critic Loss1: 1.0086e-01 | Critic Loss2: 9.6352e-02 | Actor Loss: -0.4409 | KL: 0.2802 | Time: 1585.0s\n",
      "[ 28000] Updated reference policy\n",
      "[ 29000] Critic Loss1: 4.0344e-02 | Critic Loss2: 3.5097e-02 | Actor Loss: -0.4719 | KL: 0.2843 | Time: 1642.6s\n",
      "[ 29000] Updated reference policy\n",
      "[ 30000] Critic Loss1: 2.9512e-02 | Critic Loss2: 3.1211e-02 | Actor Loss: -0.4810 | KL: 0.2565 | Time: 1700.0s\n",
      "[ 30000] Updated reference policy\n",
      "[ 30000] ===== EVAL: 208.71 =====\n",
      "[ 31000] Critic Loss1: 3.3706e-02 | Critic Loss2: 2.7619e-02 | Actor Loss: -0.4328 | KL: 0.2384 | Time: 1778.5s\n",
      "[ 31000] Updated reference policy\n",
      "[ 32000] Critic Loss1: 3.0246e-01 | Critic Loss2: 3.0857e-01 | Actor Loss: -0.4973 | KL: 0.2381 | Time: 1836.0s\n",
      "[ 32000] Updated reference policy\n",
      "[ 33000] Critic Loss1: 2.3186e-01 | Critic Loss2: 2.3200e-01 | Actor Loss: -0.4652 | KL: 0.3055 | Time: 1893.4s\n",
      "[ 33000] Updated reference policy\n",
      "[ 34000] Critic Loss1: 2.6761e-02 | Critic Loss2: 2.4784e-02 | Actor Loss: -0.4846 | KL: 0.2955 | Time: 1950.9s\n",
      "[ 34000] Updated reference policy\n",
      "[ 35000] Critic Loss1: 2.4217e-02 | Critic Loss2: 2.7026e-02 | Actor Loss: -0.5157 | KL: 0.3157 | Time: 2008.2s\n",
      "[ 35000] Updated reference policy\n",
      "[ 35000] ===== EVAL: 246.70 =====\n",
      "[ 36000] Critic Loss1: 2.1707e-02 | Critic Loss2: 2.8886e-02 | Actor Loss: -0.4757 | KL: 0.2853 | Time: 2086.5s\n",
      "[ 36000] Updated reference policy\n",
      "Step  36001 | Episode   30 | Reward:   467.93 | Length: 2000 | Time: 2086.5s\n",
      "[ 37000] Critic Loss1: 2.6098e-02 | Critic Loss2: 1.9758e-02 | Actor Loss: -0.4869 | KL: 0.2342 | Time: 2144.2s\n",
      "[ 37000] Updated reference policy\n",
      "[ 38000] Critic Loss1: 2.1051e-02 | Critic Loss2: 3.1430e-02 | Actor Loss: -0.5228 | KL: 0.2986 | Time: 2202.0s\n",
      "[ 38000] Updated reference policy\n",
      "[ 39000] Critic Loss1: 3.7411e-02 | Critic Loss2: 3.8150e-02 | Actor Loss: -0.4511 | KL: 0.2996 | Time: 2259.6s\n",
      "[ 39000] Updated reference policy\n",
      "[ 40000] Critic Loss1: 1.9938e-02 | Critic Loss2: 2.2249e-02 | Actor Loss: -0.4159 | KL: 0.2811 | Time: 2317.3s\n",
      "[ 40000] Updated reference policy\n",
      "[ 40000] ===== EVAL: 203.13 =====\n",
      "[ 41000] Critic Loss1: 3.7407e-01 | Critic Loss2: 3.6974e-01 | Actor Loss: -0.4608 | KL: 0.3010 | Time: 2396.2s\n",
      "[ 41000] Updated reference policy\n",
      "[ 42000] Critic Loss1: 2.3574e-02 | Critic Loss2: 2.7854e-02 | Actor Loss: -0.4716 | KL: 0.3072 | Time: 2453.9s\n",
      "[ 42000] Updated reference policy\n",
      "[ 43000] Critic Loss1: 2.2572e-02 | Critic Loss2: 2.2808e-02 | Actor Loss: -0.5180 | KL: 0.2654 | Time: 2511.5s\n",
      "[ 43000] Updated reference policy\n",
      "[ 44000] Critic Loss1: 2.9455e-02 | Critic Loss2: 2.8711e-02 | Actor Loss: -0.3917 | KL: 0.2896 | Time: 2568.9s\n",
      "[ 44000] Updated reference policy\n",
      "[ 45000] Critic Loss1: 2.2741e-02 | Critic Loss2: 2.7165e-02 | Actor Loss: -0.5347 | KL: 0.3130 | Time: 2626.3s\n",
      "[ 45000] Updated reference policy\n",
      "[ 45000] ===== EVAL: 203.06 =====\n",
      "[ 46000] Critic Loss1: 2.8951e-02 | Critic Loss2: 2.6634e-02 | Actor Loss: -0.5134 | KL: 0.2862 | Time: 2704.6s\n",
      "[ 46000] Updated reference policy\n",
      "[ 47000] Critic Loss1: 3.7751e-02 | Critic Loss2: 3.9019e-02 | Actor Loss: -0.3909 | KL: 0.2714 | Time: 2762.2s\n",
      "[ 47000] Updated reference policy\n",
      "[ 48000] Critic Loss1: 4.2042e-01 | Critic Loss2: 4.3733e-01 | Actor Loss: -0.3764 | KL: 0.2790 | Time: 2819.7s\n",
      "[ 48000] Updated reference policy\n",
      "Step  48001 | Episode   40 | Reward:   210.87 | Length: 1000 | Time: 2819.7s\n",
      "[ 49000] Critic Loss1: 1.2949e+00 | Critic Loss2: 1.3147e+00 | Actor Loss: -0.3883 | KL: 0.2461 | Time: 2877.1s\n",
      "[ 49000] Updated reference policy\n",
      "[ 50000] Critic Loss1: 3.3757e-01 | Critic Loss2: 3.3327e-01 | Actor Loss: -0.3988 | KL: 0.2875 | Time: 2934.5s\n",
      "[ 50000] Updated reference policy\n",
      "[ 50000] ===== EVAL: 196.79 =====\n",
      "[ 51000] Critic Loss1: 2.5737e+00 | Critic Loss2: 2.5287e+00 | Actor Loss: -0.5388 | KL: 0.3056 | Time: 3014.5s\n",
      "[ 51000] Updated reference policy\n",
      "[ 52000] Critic Loss1: 2.8910e-02 | Critic Loss2: 3.6628e-02 | Actor Loss: -0.2764 | KL: 0.3446 | Time: 3072.3s\n",
      "[ 52000] Updated reference policy\n",
      "[ 53000] Critic Loss1: 2.4124e-02 | Critic Loss2: 2.6922e-02 | Actor Loss: -0.3722 | KL: 0.2452 | Time: 3129.8s\n",
      "[ 53000] Updated reference policy\n",
      "[ 54000] Critic Loss1: 4.0645e-02 | Critic Loss2: 3.2895e-02 | Actor Loss: -0.3566 | KL: 0.2266 | Time: 3187.2s\n",
      "[ 54000] Updated reference policy\n",
      "[ 55000] Critic Loss1: 6.7833e-02 | Critic Loss2: 6.7096e-02 | Actor Loss: -0.3783 | KL: 0.2191 | Time: 3244.7s\n",
      "[ 55000] Updated reference policy\n",
      "[ 55000] ===== EVAL: 192.57 =====\n",
      "[ 56000] Critic Loss1: 2.9439e-02 | Critic Loss2: 2.2957e-02 | Actor Loss: -0.4495 | KL: 0.2301 | Time: 3323.0s\n",
      "[ 56000] Updated reference policy\n",
      "[ 57000] Critic Loss1: 2.4509e-02 | Critic Loss2: 2.3522e-02 | Actor Loss: -0.4011 | KL: 0.2345 | Time: 3380.5s\n",
      "[ 57000] Updated reference policy\n",
      "[ 58000] Critic Loss1: 4.2588e-01 | Critic Loss2: 4.0455e-01 | Actor Loss: -0.3448 | KL: 0.1995 | Time: 3438.3s\n",
      "[ 58000] Updated reference policy\n",
      "[ 59000] Critic Loss1: 2.2600e-02 | Critic Loss2: 2.7966e-02 | Actor Loss: -0.3918 | KL: 0.2340 | Time: 3495.8s\n",
      "[ 59000] Updated reference policy\n",
      "[ 60000] Critic Loss1: 4.5787e-01 | Critic Loss2: 4.5098e-01 | Actor Loss: -0.2868 | KL: 0.1883 | Time: 3553.4s\n",
      "[ 60000] Updated reference policy\n",
      "[ 60000] ===== EVAL: 192.60 =====\n",
      "[ 61000] Critic Loss1: 7.0868e-02 | Critic Loss2: 6.0132e-02 | Actor Loss: -0.3444 | KL: 0.2166 | Time: 3631.5s\n",
      "[ 61000] Updated reference policy\n",
      "Step  61001 | Episode   50 | Reward:   384.90 | Length: 2000 | Time: 3631.5s\n",
      "[ 62000] Critic Loss1: 7.4757e-02 | Critic Loss2: 6.1486e-02 | Actor Loss: -0.4127 | KL: 0.2339 | Time: 3688.9s\n",
      "[ 62000] Updated reference policy\n",
      "[ 63000] Critic Loss1: 6.1799e-01 | Critic Loss2: 6.3227e-01 | Actor Loss: -0.4232 | KL: 0.2414 | Time: 3746.4s\n",
      "[ 63000] Updated reference policy\n",
      "[ 64000] Critic Loss1: 3.0329e-02 | Critic Loss2: 2.2109e-02 | Actor Loss: -0.3539 | KL: 0.1857 | Time: 3803.8s\n",
      "[ 64000] Updated reference policy\n",
      "[ 65000] Critic Loss1: 2.7738e-01 | Critic Loss2: 2.8465e-01 | Actor Loss: -0.3656 | KL: 0.2010 | Time: 3861.1s\n",
      "[ 65000] Updated reference policy\n",
      "[ 65000] ===== EVAL: 194.92 =====\n",
      "[ 66000] Critic Loss1: 2.8303e-02 | Critic Loss2: 2.4233e-02 | Actor Loss: -0.3630 | KL: 0.2178 | Time: 3940.0s\n",
      "[ 66000] Updated reference policy\n",
      "[ 67000] Critic Loss1: 2.0344e-02 | Critic Loss2: 2.3074e-02 | Actor Loss: -0.3625 | KL: 0.1948 | Time: 3997.1s\n",
      "[ 67000] Updated reference policy\n",
      "[ 68000] Critic Loss1: 1.0402e+00 | Critic Loss2: 1.0283e+00 | Actor Loss: -0.2136 | KL: 0.1562 | Time: 4054.3s\n",
      "[ 68000] Updated reference policy\n",
      "[ 69000] Critic Loss1: 3.7461e-02 | Critic Loss2: 3.7087e-02 | Actor Loss: -0.4248 | KL: 0.1657 | Time: 4111.6s\n",
      "[ 69000] Updated reference policy\n",
      "[ 70000] Critic Loss1: 2.2746e-02 | Critic Loss2: 1.9130e-02 | Actor Loss: -0.2315 | KL: 0.1742 | Time: 4168.8s\n",
      "[ 70000] Updated reference policy\n",
      "[ 70000] ===== EVAL: 197.40 =====\n",
      "[ 71000] Critic Loss1: 2.7469e-02 | Critic Loss2: 2.7915e-02 | Actor Loss: -0.2666 | KL: 0.1536 | Time: 4246.6s\n",
      "[ 71000] Updated reference policy\n",
      "[ 72000] Critic Loss1: 2.3704e-02 | Critic Loss2: 1.7537e-02 | Actor Loss: -0.3178 | KL: 0.2001 | Time: 4303.8s\n",
      "[ 72000] Updated reference policy\n",
      "[ 73000] Critic Loss1: 4.3106e-01 | Critic Loss2: 4.4522e-01 | Actor Loss: -0.2811 | KL: 0.1581 | Time: 4361.0s\n",
      "[ 73000] Updated reference policy\n",
      "Step  73001 | Episode   60 | Reward:   200.37 | Length: 1000 | Time: 4361.0s\n",
      "[ 74000] Critic Loss1: 5.7519e-01 | Critic Loss2: 5.8290e-01 | Actor Loss: -0.3472 | KL: 0.1696 | Time: 4418.4s\n",
      "[ 74000] Updated reference policy\n",
      "[ 75000] Critic Loss1: 1.1350e-01 | Critic Loss2: 1.0801e-01 | Actor Loss: -0.2324 | KL: 0.1632 | Time: 4475.6s\n",
      "[ 75000] Updated reference policy\n",
      "[ 75000] ===== EVAL: 191.66 =====\n",
      "[ 76000] Critic Loss1: 2.1161e-02 | Critic Loss2: 2.0813e-02 | Actor Loss: -0.3048 | KL: 0.1854 | Time: 4554.0s\n",
      "[ 76000] Updated reference policy\n",
      "[ 77000] Critic Loss1: 2.2096e-02 | Critic Loss2: 2.4085e-02 | Actor Loss: -0.3660 | KL: 0.1551 | Time: 4611.3s\n",
      "[ 77000] Updated reference policy\n",
      "[ 78000] Critic Loss1: 1.4213e-02 | Critic Loss2: 1.3952e-02 | Actor Loss: -0.3453 | KL: 0.1575 | Time: 4668.6s\n",
      "[ 78000] Updated reference policy\n",
      "[ 79000] Critic Loss1: 1.6327e-02 | Critic Loss2: 2.0982e-02 | Actor Loss: -0.2873 | KL: 0.1908 | Time: 4725.9s\n",
      "[ 79000] Updated reference policy\n",
      "[ 80000] Critic Loss1: 1.6468e-02 | Critic Loss2: 2.4050e-02 | Actor Loss: -0.2298 | KL: 0.1645 | Time: 4783.2s\n",
      "[ 80000] Updated reference policy\n",
      "[ 80000] ===== EVAL: 196.20 =====\n",
      "[ 81000] Critic Loss1: 6.2276e-01 | Critic Loss2: 6.1821e-01 | Actor Loss: -0.3673 | KL: 0.1568 | Time: 4861.4s\n",
      "[ 81000] Updated reference policy\n",
      "[ 82000] Critic Loss1: 1.9516e-02 | Critic Loss2: 1.6168e-02 | Actor Loss: -0.2446 | KL: 0.1812 | Time: 4918.1s\n",
      "[ 82000] Updated reference policy\n",
      "[ 83000] Critic Loss1: 1.6001e-02 | Critic Loss2: 1.6858e-02 | Actor Loss: -0.3291 | KL: 0.1758 | Time: 4975.5s\n",
      "[ 83000] Updated reference policy\n",
      "[ 84000] Critic Loss1: 2.4770e-02 | Critic Loss2: 2.2871e-02 | Actor Loss: -0.3494 | KL: 0.2262 | Time: 5032.9s\n",
      "[ 84000] Updated reference policy\n",
      "[ 85000] Critic Loss1: 2.5824e-02 | Critic Loss2: 2.3303e-02 | Actor Loss: -0.3745 | KL: 0.1635 | Time: 5090.4s\n",
      "[ 85000] Updated reference policy\n",
      "[ 85000] ===== EVAL: 199.88 =====\n",
      "[ 86000] Critic Loss1: 2.5989e-02 | Critic Loss2: 2.5099e-02 | Actor Loss: -0.2872 | KL: 0.1494 | Time: 5168.5s\n",
      "[ 86000] Updated reference policy\n",
      "Step  86001 | Episode   70 | Reward:   391.02 | Length: 2000 | Time: 5168.5s\n",
      "[ 87000] Critic Loss1: 2.6289e-02 | Critic Loss2: 2.3100e-02 | Actor Loss: -0.2319 | KL: 0.1471 | Time: 5225.7s\n",
      "[ 87000] Updated reference policy\n",
      "[ 88000] Critic Loss1: 7.1769e-02 | Critic Loss2: 7.6723e-02 | Actor Loss: -0.2585 | KL: 0.1640 | Time: 5282.9s\n",
      "[ 88000] Updated reference policy\n",
      "[ 89000] Critic Loss1: 1.6297e-02 | Critic Loss2: 1.9541e-02 | Actor Loss: -0.3025 | KL: 0.1912 | Time: 5340.1s\n",
      "[ 89000] Updated reference policy\n",
      "[ 90000] Critic Loss1: 2.6240e-02 | Critic Loss2: 2.0472e-02 | Actor Loss: -0.3383 | KL: 0.1694 | Time: 5397.3s\n",
      "[ 90000] Updated reference policy\n",
      "[ 90000] ===== EVAL: 198.21 =====\n",
      "[ 91000] Critic Loss1: 1.1857e+00 | Critic Loss2: 1.1997e+00 | Actor Loss: -0.3430 | KL: 0.1501 | Time: 5475.5s\n",
      "[ 91000] Updated reference policy\n",
      "[ 92000] Critic Loss1: 2.4928e-02 | Critic Loss2: 1.9417e-02 | Actor Loss: -0.2983 | KL: 0.2301 | Time: 5532.8s\n",
      "[ 92000] Updated reference policy\n",
      "[ 93000] Critic Loss1: 2.1046e-02 | Critic Loss2: 2.0567e-02 | Actor Loss: -0.3191 | KL: 0.1860 | Time: 5590.1s\n",
      "[ 93000] Updated reference policy\n",
      "[ 94000] Critic Loss1: 2.5668e-02 | Critic Loss2: 2.5689e-02 | Actor Loss: -0.2926 | KL: 0.1651 | Time: 5647.3s\n",
      "[ 94000] Updated reference policy\n",
      "[ 95000] Critic Loss1: 4.8868e-01 | Critic Loss2: 4.9435e-01 | Actor Loss: -0.3476 | KL: 0.1925 | Time: 5704.5s\n",
      "[ 95000] Updated reference policy\n",
      "[ 95000] ===== EVAL: 212.48 =====\n",
      "[ 96000] Critic Loss1: 2.3530e-02 | Critic Loss2: 1.9667e-02 | Actor Loss: -0.3469 | KL: 0.1522 | Time: 5782.5s\n",
      "[ 96000] Updated reference policy\n",
      "[ 97000] Critic Loss1: 2.1755e-02 | Critic Loss2: 1.9905e-02 | Actor Loss: -0.3856 | KL: 0.2059 | Time: 5839.7s\n",
      "[ 97000] Updated reference policy\n",
      "[ 98000] Critic Loss1: 1.9085e-02 | Critic Loss2: 1.9709e-02 | Actor Loss: -0.2900 | KL: 0.2127 | Time: 5896.9s\n",
      "[ 98000] Updated reference policy\n",
      "Step  98001 | Episode   80 | Reward:   204.88 | Length: 1000 | Time: 5896.9s\n",
      "[ 99000] Critic Loss1: 3.0867e-02 | Critic Loss2: 2.1170e-02 | Actor Loss: -0.3119 | KL: 0.2005 | Time: 5954.1s\n",
      "[ 99000] Updated reference policy\n",
      "[100000] Critic Loss1: 3.6312e-01 | Critic Loss2: 3.5714e-01 | Actor Loss: -0.3406 | KL: 0.2045 | Time: 6011.3s\n",
      "[100000] Updated reference policy\n",
      "[100000] ===== EVAL: 211.44 =====\n",
      "[101000] Critic Loss1: 1.9508e-02 | Critic Loss2: 2.1839e-02 | Actor Loss: -0.2158 | KL: 0.2084 | Time: 6089.4s\n",
      "[101000] Updated reference policy\n",
      "[102000] Critic Loss1: 2.2839e-02 | Critic Loss2: 2.5446e-02 | Actor Loss: -0.4006 | KL: 0.2138 | Time: 6146.5s\n",
      "[102000] Updated reference policy\n",
      "[103000] Critic Loss1: 1.9092e-02 | Critic Loss2: 1.6967e-02 | Actor Loss: -0.3454 | KL: 0.2166 | Time: 6203.6s\n",
      "[103000] Updated reference policy\n",
      "[104000] Critic Loss1: 5.3892e-01 | Critic Loss2: 5.5228e-01 | Actor Loss: -0.3571 | KL: 0.1910 | Time: 6260.8s\n",
      "[104000] Updated reference policy\n",
      "[105000] Critic Loss1: 3.3370e-02 | Critic Loss2: 1.9176e-02 | Actor Loss: -0.3267 | KL: 0.2264 | Time: 6318.0s\n",
      "[105000] Updated reference policy\n",
      "[105000] ===== EVAL: 232.30 =====\n",
      "[106000] Critic Loss1: 2.4016e-02 | Critic Loss2: 2.1387e-02 | Actor Loss: -0.2564 | KL: 0.1961 | Time: 6396.1s\n",
      "[106000] Updated reference policy\n",
      "[107000] Critic Loss1: 4.4981e-02 | Critic Loss2: 4.4381e-02 | Actor Loss: -0.2485 | KL: 0.1940 | Time: 6453.2s\n",
      "[107000] Updated reference policy\n",
      "[108000] Critic Loss1: 1.8475e-02 | Critic Loss2: 1.9077e-02 | Actor Loss: -0.3065 | KL: 0.1786 | Time: 6510.4s\n",
      "[108000] Updated reference policy\n",
      "[109000] Critic Loss1: 4.1108e-01 | Critic Loss2: 4.1165e-01 | Actor Loss: -0.3511 | KL: 0.2052 | Time: 6567.7s\n",
      "[109000] Updated reference policy\n",
      "[110000] Critic Loss1: 2.6060e-02 | Critic Loss2: 2.8183e-02 | Actor Loss: -0.4258 | KL: 0.1967 | Time: 6625.0s\n",
      "[110000] Updated reference policy\n",
      "[110000] ===== EVAL: 246.07 =====\n",
      "[111000] Critic Loss1: 9.0250e-02 | Critic Loss2: 6.7276e-02 | Actor Loss: -0.2699 | KL: 0.1786 | Time: 6703.3s\n",
      "[111000] Updated reference policy\n",
      "Step 111001 | Episode   90 | Reward:   485.19 | Length: 2000 | Time: 6703.3s\n",
      "[112000] Critic Loss1: 4.6432e-01 | Critic Loss2: 4.8211e-01 | Actor Loss: -0.3307 | KL: 0.2089 | Time: 6760.7s\n",
      "[112000] Updated reference policy\n",
      "[113000] Critic Loss1: 1.0518e+00 | Critic Loss2: 1.0634e+00 | Actor Loss: -0.4080 | KL: 0.1941 | Time: 6818.0s\n",
      "[113000] Updated reference policy\n",
      "[114000] Critic Loss1: 3.0146e-02 | Critic Loss2: 3.0961e-02 | Actor Loss: -0.2288 | KL: 0.1862 | Time: 6875.3s\n",
      "[114000] Updated reference policy\n",
      "[115000] Critic Loss1: 2.0394e-02 | Critic Loss2: 2.5160e-02 | Actor Loss: -0.3293 | KL: 0.2127 | Time: 6932.5s\n",
      "[115000] Updated reference policy\n",
      "[115000] ===== EVAL: 267.38 =====\n",
      "[116000] Critic Loss1: 2.8600e-02 | Critic Loss2: 2.7081e-02 | Actor Loss: -0.3039 | KL: 0.2162 | Time: 7010.6s\n",
      "[116000] Updated reference policy\n",
      "[117000] Critic Loss1: 3.6399e-01 | Critic Loss2: 3.6126e-01 | Actor Loss: -0.2784 | KL: 0.2004 | Time: 7067.9s\n",
      "[117000] Updated reference policy\n",
      "[118000] Critic Loss1: 2.3613e-02 | Critic Loss2: 2.2801e-02 | Actor Loss: -0.3153 | KL: 0.1790 | Time: 7125.1s\n",
      "[118000] Updated reference policy\n",
      "[119000] Critic Loss1: 3.0111e-02 | Critic Loss2: 3.6179e-02 | Actor Loss: -0.2967 | KL: 0.1981 | Time: 7182.3s\n",
      "[119000] Updated reference policy\n",
      "[120000] Critic Loss1: 2.9587e-02 | Critic Loss2: 3.2141e-02 | Actor Loss: -0.3674 | KL: 0.1801 | Time: 7239.4s\n",
      "[120000] Updated reference policy\n",
      "[120000] ===== EVAL: 238.15 =====\n",
      "[121000] Critic Loss1: 2.7912e-02 | Critic Loss2: 2.5494e-02 | Actor Loss: -0.3807 | KL: 0.2138 | Time: 7317.4s\n",
      "[121000] Updated reference policy\n",
      "[122000] Critic Loss1: 3.3594e-02 | Critic Loss2: 2.6448e-02 | Actor Loss: -0.3404 | KL: 0.1889 | Time: 7374.8s\n",
      "[122000] Updated reference policy\n",
      "[123000] Critic Loss1: 3.5406e-02 | Critic Loss2: 3.6381e-02 | Actor Loss: -0.3065 | KL: 0.2197 | Time: 7432.2s\n",
      "[123000] Updated reference policy\n",
      "Step 123001 | Episode  100 | Reward:   260.41 | Length: 1000 | Time: 7432.2s\n",
      "[124000] Critic Loss1: 3.9937e-02 | Critic Loss2: 3.7789e-02 | Actor Loss: -0.4505 | KL: 0.1963 | Time: 7489.6s\n",
      "[124000] Updated reference policy\n",
      "[125000] Critic Loss1: 2.5778e-02 | Critic Loss2: 3.8813e-02 | Actor Loss: -0.3395 | KL: 0.2276 | Time: 7547.0s\n",
      "[125000] Updated reference policy\n",
      "[125000] ===== EVAL: 267.11 =====\n",
      "[126000] Critic Loss1: 1.9535e-01 | Critic Loss2: 2.0391e-01 | Actor Loss: -0.3522 | KL: 0.2561 | Time: 7625.6s\n",
      "[126000] Updated reference policy\n",
      "[127000] Critic Loss1: 2.7000e-01 | Critic Loss2: 2.5685e-01 | Actor Loss: -0.3071 | KL: 0.2293 | Time: 7683.0s\n",
      "[127000] Updated reference policy\n",
      "[128000] Critic Loss1: 5.3508e-02 | Critic Loss2: 3.1078e-02 | Actor Loss: -0.3594 | KL: 0.2151 | Time: 7740.3s\n",
      "[128000] Updated reference policy\n",
      "[129000] Critic Loss1: 8.8960e-01 | Critic Loss2: 9.1308e-01 | Actor Loss: -0.3708 | KL: 0.2191 | Time: 7797.4s\n",
      "[129000] Updated reference policy\n",
      "[130000] Critic Loss1: 3.3254e-02 | Critic Loss2: 3.6383e-02 | Actor Loss: -0.3488 | KL: 0.2327 | Time: 7854.7s\n",
      "[130000] Updated reference policy\n",
      "[130000] ===== EVAL: 277.06 =====\n",
      "[131000] Critic Loss1: 4.5169e-01 | Critic Loss2: 4.5425e-01 | Actor Loss: -0.3378 | KL: 0.2138 | Time: 7933.0s\n",
      "[131000] Updated reference policy\n",
      "[132000] Critic Loss1: 3.6203e-02 | Critic Loss2: 2.9217e-02 | Actor Loss: -0.3027 | KL: 0.1992 | Time: 7990.4s\n",
      "[132000] Updated reference policy\n",
      "[133000] Critic Loss1: 3.0865e-02 | Critic Loss2: 4.0765e-02 | Actor Loss: -0.2691 | KL: 0.2087 | Time: 8047.7s\n",
      "[133000] Updated reference policy\n",
      "[134000] Critic Loss1: 6.0565e-02 | Critic Loss2: 6.5142e-02 | Actor Loss: -0.3990 | KL: 0.1987 | Time: 8105.0s\n",
      "[134000] Updated reference policy\n",
      "[135000] Critic Loss1: 3.4421e-02 | Critic Loss2: 4.9788e-02 | Actor Loss: -0.2902 | KL: 0.1798 | Time: 8162.3s\n",
      "[135000] Updated reference policy\n",
      "[135000] ===== EVAL: 272.13 =====\n",
      "[136000] Critic Loss1: 5.7418e-02 | Critic Loss2: 5.1196e-02 | Actor Loss: -0.3158 | KL: 0.1832 | Time: 8240.9s\n",
      "[136000] Updated reference policy\n",
      "Step 136001 | Episode  110 | Reward:   519.27 | Length: 2000 | Time: 8240.9s\n",
      "[137000] Critic Loss1: 5.6583e-02 | Critic Loss2: 4.1477e-02 | Actor Loss: -0.3260 | KL: 0.1951 | Time: 8298.2s\n",
      "[137000] Updated reference policy\n",
      "[138000] Critic Loss1: 3.2348e-02 | Critic Loss2: 2.6867e-02 | Actor Loss: -0.3479 | KL: 0.2609 | Time: 8355.4s\n",
      "[138000] Updated reference policy\n",
      "[139000] Critic Loss1: 9.7399e-02 | Critic Loss2: 8.8060e-02 | Actor Loss: -0.3395 | KL: 0.2179 | Time: 8412.8s\n",
      "[139000] Updated reference policy\n",
      "[140000] Critic Loss1: 5.4482e-01 | Critic Loss2: 5.4997e-01 | Actor Loss: -0.3342 | KL: 0.2321 | Time: 8470.6s\n",
      "[140000] Updated reference policy\n",
      "[140000] ===== EVAL: 267.92 =====\n",
      "[141000] Critic Loss1: 9.5691e-02 | Critic Loss2: 9.5499e-02 | Actor Loss: -0.4452 | KL: 0.2256 | Time: 8549.1s\n",
      "[141000] Updated reference policy\n",
      "[142000] Critic Loss1: 4.2893e-02 | Critic Loss2: 4.7259e-02 | Actor Loss: -0.3295 | KL: 0.1934 | Time: 8606.4s\n",
      "[142000] Updated reference policy\n",
      "[143000] Critic Loss1: 4.1897e-02 | Critic Loss2: 4.3031e-02 | Actor Loss: -0.3012 | KL: 0.1964 | Time: 8663.6s\n",
      "[143000] Updated reference policy\n",
      "[144000] Critic Loss1: 3.1028e+00 | Critic Loss2: 3.0827e+00 | Actor Loss: -0.3378 | KL: 0.2064 | Time: 8720.9s\n",
      "[144000] Updated reference policy\n",
      "[145000] Critic Loss1: 6.5490e-02 | Critic Loss2: 5.7204e-02 | Actor Loss: -0.2406 | KL: 0.2179 | Time: 8778.7s\n",
      "[145000] Updated reference policy\n",
      "[145000] ===== EVAL: 292.04 =====\n",
      "[146000] Critic Loss1: 4.0303e-02 | Critic Loss2: 3.3395e-02 | Actor Loss: -0.3978 | KL: 0.2249 | Time: 8857.9s\n",
      "[146000] Updated reference policy\n",
      "[147000] Critic Loss1: 2.2291e-01 | Critic Loss2: 2.4154e-01 | Actor Loss: -0.3547 | KL: 0.2078 | Time: 8915.6s\n",
      "[147000] Updated reference policy\n",
      "[148000] Critic Loss1: 4.7946e-02 | Critic Loss2: 4.0213e-02 | Actor Loss: -0.2860 | KL: 0.2272 | Time: 8973.2s\n",
      "[148000] Updated reference policy\n",
      "Step 148001 | Episode  120 | Reward:   269.78 | Length: 1000 | Time: 8973.2s\n",
      "[149000] Critic Loss1: 1.3862e+00 | Critic Loss2: 1.3950e+00 | Actor Loss: -0.3631 | KL: 0.2502 | Time: 9030.7s\n",
      "[149000] Updated reference policy\n",
      "[150000] Critic Loss1: 4.7924e-02 | Critic Loss2: 4.0011e-02 | Actor Loss: -0.4186 | KL: 0.2310 | Time: 9088.0s\n",
      "[150000] Updated reference policy\n",
      "[150000] ===== EVAL: 256.99 =====\n",
      "[151000] Critic Loss1: 7.3936e-02 | Critic Loss2: 5.3859e-02 | Actor Loss: -0.2835 | KL: 0.2480 | Time: 9166.2s\n",
      "[151000] Updated reference policy\n",
      "[152000] Critic Loss1: 5.8785e-02 | Critic Loss2: 4.2538e-02 | Actor Loss: -0.3254 | KL: 0.1915 | Time: 9223.4s\n",
      "[152000] Updated reference policy\n",
      "[153000] Critic Loss1: 4.6411e-01 | Critic Loss2: 4.4115e-01 | Actor Loss: -0.4018 | KL: 0.2576 | Time: 9280.8s\n",
      "[153000] Updated reference policy\n",
      "[154000] Critic Loss1: 4.8645e-02 | Critic Loss2: 4.4736e-02 | Actor Loss: -0.2893 | KL: 0.2080 | Time: 9338.1s\n",
      "[154000] Updated reference policy\n",
      "[155000] Critic Loss1: 7.2126e-02 | Critic Loss2: 4.4974e-02 | Actor Loss: -0.3547 | KL: 0.2377 | Time: 9395.5s\n",
      "[155000] Updated reference policy\n",
      "[155000] ===== EVAL: 262.58 =====\n",
      "[156000] Critic Loss1: 5.9976e-02 | Critic Loss2: 6.1944e-02 | Actor Loss: -0.3079 | KL: 0.2355 | Time: 9473.6s\n",
      "[156000] Updated reference policy\n",
      "[157000] Critic Loss1: 4.8098e-01 | Critic Loss2: 5.3006e-01 | Actor Loss: -0.3185 | KL: 0.1995 | Time: 9530.8s\n",
      "[157000] Updated reference policy\n",
      "[158000] Critic Loss1: 5.1675e-02 | Critic Loss2: 4.8884e-02 | Actor Loss: -0.2754 | KL: 0.2075 | Time: 9588.1s\n",
      "[158000] Updated reference policy\n",
      "[159000] Critic Loss1: 5.2107e-02 | Critic Loss2: 4.5753e-02 | Actor Loss: -0.3711 | KL: 0.2451 | Time: 9645.4s\n",
      "[159000] Updated reference policy\n",
      "[160000] Critic Loss1: 4.6562e-01 | Critic Loss2: 4.7781e-01 | Actor Loss: -0.3550 | KL: 0.2552 | Time: 9702.6s\n",
      "[160000] Updated reference policy\n",
      "[160000] ===== EVAL: 258.55 =====\n",
      "[161000] Critic Loss1: 6.2975e-02 | Critic Loss2: 6.6107e-02 | Actor Loss: -0.3216 | KL: 0.2236 | Time: 9780.7s\n",
      "[161000] Updated reference policy\n",
      "Step 161001 | Episode  130 | Reward:   507.27 | Length: 2000 | Time: 9780.7s\n",
      "[162000] Critic Loss1: 6.1035e-02 | Critic Loss2: 7.6054e-02 | Actor Loss: -0.3134 | KL: 0.2114 | Time: 9837.9s\n",
      "[162000] Updated reference policy\n",
      "[163000] Critic Loss1: 5.8624e-02 | Critic Loss2: 4.8570e-02 | Actor Loss: -0.4079 | KL: 0.2539 | Time: 9895.1s\n",
      "[163000] Updated reference policy\n",
      "[164000] Critic Loss1: 5.7115e-02 | Critic Loss2: 5.1598e-02 | Actor Loss: -0.4376 | KL: 0.2971 | Time: 9952.3s\n",
      "[164000] Updated reference policy\n",
      "[165000] Critic Loss1: 6.3743e-02 | Critic Loss2: 5.7829e-02 | Actor Loss: -0.3794 | KL: 0.2338 | Time: 10009.4s\n",
      "[165000] Updated reference policy\n",
      "[165000] ===== EVAL: 254.56 =====\n",
      "[166000] Critic Loss1: 7.5344e-02 | Critic Loss2: 7.9891e-02 | Actor Loss: -0.3287 | KL: 0.2173 | Time: 10087.4s\n",
      "[166000] Updated reference policy\n",
      "[167000] Critic Loss1: 5.3278e-02 | Critic Loss2: 5.9724e-02 | Actor Loss: -0.4172 | KL: 0.2633 | Time: 10144.7s\n",
      "[167000] Updated reference policy\n",
      "[168000] Critic Loss1: 5.0618e-02 | Critic Loss2: 6.1232e-02 | Actor Loss: -0.3963 | KL: 0.2780 | Time: 10202.1s\n",
      "[168000] Updated reference policy\n",
      "[169000] Critic Loss1: 4.0042e-02 | Critic Loss2: 4.3510e-02 | Actor Loss: -0.4848 | KL: 0.2568 | Time: 10258.9s\n",
      "[169000] Updated reference policy\n",
      "[170000] Critic Loss1: 5.5051e-02 | Critic Loss2: 7.4781e-02 | Actor Loss: -0.3223 | KL: 0.2259 | Time: 10316.2s\n",
      "[170000] Updated reference policy\n",
      "[170000] ===== EVAL: 261.21 =====\n",
      "[171000] Critic Loss1: 7.1803e-02 | Critic Loss2: 7.1571e-02 | Actor Loss: -0.3119 | KL: 0.2434 | Time: 10394.1s\n",
      "[171000] Updated reference policy\n",
      "[172000] Critic Loss1: 7.9236e-02 | Critic Loss2: 6.3153e-02 | Actor Loss: -0.3307 | KL: 0.2613 | Time: 10456.5s\n",
      "[172000] Updated reference policy\n",
      "[173000] Critic Loss1: 6.5207e-01 | Critic Loss2: 6.2424e-01 | Actor Loss: -0.4621 | KL: 0.2466 | Time: 10513.8s\n",
      "[173000] Updated reference policy\n",
      "Step 173001 | Episode  140 | Reward:   267.38 | Length: 1000 | Time: 10513.8s\n",
      "[174000] Critic Loss1: 6.9474e-02 | Critic Loss2: 7.0441e-02 | Actor Loss: -0.3251 | KL: 0.2356 | Time: 10571.3s\n",
      "[174000] Updated reference policy\n",
      "[175000] Critic Loss1: 5.9445e-02 | Critic Loss2: 5.7950e-02 | Actor Loss: -0.3743 | KL: 0.2325 | Time: 10628.5s\n",
      "[175000] Updated reference policy\n",
      "[175000] ===== EVAL: 244.61 =====\n",
      "[176000] Critic Loss1: 8.2672e-02 | Critic Loss2: 7.3009e-02 | Actor Loss: -0.3829 | KL: 0.2242 | Time: 10706.2s\n",
      "[176000] Updated reference policy\n",
      "[177000] Critic Loss1: 8.5469e-02 | Critic Loss2: 7.5955e-02 | Actor Loss: -0.2487 | KL: 0.2301 | Time: 10763.3s\n",
      "[177000] Updated reference policy\n",
      "[178000] Critic Loss1: 6.7832e-02 | Critic Loss2: 6.4702e-02 | Actor Loss: -0.3876 | KL: 0.2332 | Time: 10820.5s\n",
      "[178000] Updated reference policy\n",
      "[179000] Critic Loss1: 1.0523e-01 | Critic Loss2: 9.0759e-02 | Actor Loss: -0.3361 | KL: 0.2311 | Time: 10877.6s\n",
      "[179000] Updated reference policy\n",
      "[180000] Critic Loss1: 5.1074e-02 | Critic Loss2: 6.6208e-02 | Actor Loss: -0.2610 | KL: 0.2067 | Time: 10934.7s\n",
      "[180000] Updated reference policy\n",
      "[180000] ===== EVAL: 263.69 =====\n",
      "[181000] Critic Loss1: 6.8339e-02 | Critic Loss2: 5.8518e-02 | Actor Loss: -0.4372 | KL: 0.2776 | Time: 11012.2s\n",
      "[181000] Updated reference policy\n",
      "[182000] Critic Loss1: 7.2401e-02 | Critic Loss2: 6.8121e-02 | Actor Loss: -0.2653 | KL: 0.2316 | Time: 11069.3s\n",
      "[182000] Updated reference policy\n",
      "[183000] Critic Loss1: 8.8904e-02 | Critic Loss2: 9.5396e-02 | Actor Loss: -0.3723 | KL: 0.2119 | Time: 11126.5s\n",
      "[183000] Updated reference policy\n",
      "[184000] Critic Loss1: 6.1917e-02 | Critic Loss2: 6.8314e-02 | Actor Loss: -0.3556 | KL: 0.2326 | Time: 11183.6s\n",
      "[184000] Updated reference policy\n",
      "[185000] Critic Loss1: 7.4392e-02 | Critic Loss2: 8.9649e-02 | Actor Loss: -0.3059 | KL: 0.2114 | Time: 11240.8s\n",
      "[185000] Updated reference policy\n",
      "[185000] ===== EVAL: 258.48 =====\n",
      "[186000] Critic Loss1: 1.0252e-01 | Critic Loss2: 7.1512e-02 | Actor Loss: -0.3150 | KL: 0.1725 | Time: 11318.6s\n",
      "[186000] Updated reference policy\n",
      "Step 186001 | Episode  150 | Reward:   495.40 | Length: 2000 | Time: 11318.6s\n",
      "[187000] Critic Loss1: 1.0415e-01 | Critic Loss2: 1.0182e-01 | Actor Loss: -0.3231 | KL: 0.1741 | Time: 11375.8s\n",
      "[187000] Updated reference policy\n",
      "[188000] Critic Loss1: 1.1557e-01 | Critic Loss2: 1.2033e-01 | Actor Loss: -0.3419 | KL: 0.2027 | Time: 11433.0s\n",
      "[188000] Updated reference policy\n",
      "[189000] Critic Loss1: 6.6124e-02 | Critic Loss2: 6.5793e-02 | Actor Loss: -0.3340 | KL: 0.2452 | Time: 11490.1s\n",
      "[189000] Updated reference policy\n",
      "[190000] Critic Loss1: 5.1234e-02 | Critic Loss2: 7.0025e-02 | Actor Loss: -0.2796 | KL: 0.2322 | Time: 11547.2s\n",
      "[190000] Updated reference policy\n",
      "[190000] ===== EVAL: 246.56 =====\n",
      "[191000] Critic Loss1: 7.7678e-02 | Critic Loss2: 5.6852e-02 | Actor Loss: -0.2826 | KL: 0.2042 | Time: 11625.2s\n",
      "[191000] Updated reference policy\n",
      "[192000] Critic Loss1: 6.9298e-02 | Critic Loss2: 6.0091e-02 | Actor Loss: -0.3605 | KL: 0.2081 | Time: 11681.6s\n",
      "[192000] Updated reference policy\n",
      "[193000] Critic Loss1: 8.0302e-02 | Critic Loss2: 6.9156e-02 | Actor Loss: -0.3133 | KL: 0.1824 | Time: 11737.5s\n",
      "[193000] Updated reference policy\n",
      "[194000] Critic Loss1: 1.0843e-01 | Critic Loss2: 1.3861e-01 | Actor Loss: -0.3513 | KL: 0.1902 | Time: 11794.3s\n",
      "[194000] Updated reference policy\n",
      "[195000] Critic Loss1: 7.3212e-02 | Critic Loss2: 6.6098e-02 | Actor Loss: -0.2985 | KL: 0.2118 | Time: 11850.9s\n",
      "[195000] Updated reference policy\n",
      "[195000] ===== EVAL: 256.45 =====\n",
      "[196000] Critic Loss1: 6.7007e-02 | Critic Loss2: 6.7033e-02 | Actor Loss: -0.2461 | KL: 0.2115 | Time: 11928.5s\n",
      "[196000] Updated reference policy\n",
      "[197000] Critic Loss1: 2.0836e-01 | Critic Loss2: 3.2339e-01 | Actor Loss: -0.3484 | KL: 0.2205 | Time: 11985.7s\n",
      "[197000] Updated reference policy\n",
      "[198000] Critic Loss1: 8.7444e-02 | Critic Loss2: 6.0335e-02 | Actor Loss: -0.3282 | KL: 0.2233 | Time: 12042.7s\n",
      "[198000] Updated reference policy\n",
      "Step 198001 | Episode  160 | Reward:   243.87 | Length: 1000 | Time: 12042.7s\n",
      "[199000] Critic Loss1: 8.0053e-02 | Critic Loss2: 6.8130e-02 | Actor Loss: -0.3260 | KL: 0.2009 | Time: 12099.8s\n",
      "[199000] Updated reference policy\n",
      "[200000] Critic Loss1: 1.0457e-01 | Critic Loss2: 1.0206e-01 | Actor Loss: -0.3796 | KL: 0.1700 | Time: 12156.8s\n",
      "[200000] Updated reference policy\n",
      "[200000] ===== EVAL: 272.04 =====\n",
      "[201000] Critic Loss1: 5.9718e-02 | Critic Loss2: 7.4008e-02 | Actor Loss: -0.3253 | KL: 0.1896 | Time: 12234.5s\n",
      "[201000] Updated reference policy\n",
      "[202000] Critic Loss1: 8.5667e-02 | Critic Loss2: 9.4111e-02 | Actor Loss: -0.2300 | KL: 0.1957 | Time: 12291.6s\n",
      "[202000] Updated reference policy\n",
      "[203000] Critic Loss1: 1.1796e-01 | Critic Loss2: 1.1333e-01 | Actor Loss: -0.3416 | KL: 0.2218 | Time: 12348.6s\n",
      "[203000] Updated reference policy\n",
      "[204000] Critic Loss1: 1.8826e-01 | Critic Loss2: 1.8596e-01 | Actor Loss: -0.2917 | KL: 0.1937 | Time: 12405.6s\n",
      "[204000] Updated reference policy\n",
      "[205000] Critic Loss1: 7.4830e-02 | Critic Loss2: 5.7708e-02 | Actor Loss: -0.3280 | KL: 0.2398 | Time: 12462.6s\n",
      "[205000] Updated reference policy\n",
      "[205000] ===== EVAL: 265.34 =====\n",
      "[206000] Critic Loss1: 1.5745e-01 | Critic Loss2: 1.5747e-01 | Actor Loss: -0.3355 | KL: 0.1999 | Time: 12540.4s\n",
      "[206000] Updated reference policy\n",
      "[207000] Critic Loss1: 1.0408e+00 | Critic Loss2: 1.0688e+00 | Actor Loss: -0.3309 | KL: 0.2375 | Time: 12597.6s\n",
      "[207000] Updated reference policy\n",
      "[208000] Critic Loss1: 8.7594e-02 | Critic Loss2: 9.5506e-02 | Actor Loss: -0.3340 | KL: 0.2379 | Time: 12654.9s\n",
      "[208000] Updated reference policy\n",
      "[209000] Critic Loss1: 7.7104e-02 | Critic Loss2: 7.8297e-02 | Actor Loss: -0.3248 | KL: 0.1800 | Time: 12712.1s\n",
      "[209000] Updated reference policy\n",
      "[210000] Critic Loss1: 9.6065e-02 | Critic Loss2: 9.8997e-02 | Actor Loss: -0.2881 | KL: 0.1644 | Time: 12769.2s\n",
      "[210000] Updated reference policy\n",
      "[210000] ===== EVAL: 274.91 =====\n",
      "[211000] Critic Loss1: 1.0051e+00 | Critic Loss2: 1.0262e+00 | Actor Loss: -0.2465 | KL: 0.2423 | Time: 12846.8s\n",
      "[211000] Updated reference policy\n",
      "Step 211001 | Episode  170 | Reward:   598.50 | Length: 2000 | Time: 12846.8s\n",
      "[212000] Critic Loss1: 8.0705e-02 | Critic Loss2: 7.1038e-02 | Actor Loss: -0.1996 | KL: 0.1981 | Time: 12903.9s\n",
      "[212000] Updated reference policy\n",
      "[213000] Critic Loss1: 1.0061e+00 | Critic Loss2: 1.0028e+00 | Actor Loss: -0.2624 | KL: 0.2133 | Time: 12961.0s\n",
      "[213000] Updated reference policy\n",
      "[214000] Critic Loss1: 2.5514e-01 | Critic Loss2: 2.3914e-01 | Actor Loss: -0.2952 | KL: 0.2371 | Time: 13018.1s\n",
      "[214000] Updated reference policy\n",
      "[215000] Critic Loss1: 1.4573e-01 | Critic Loss2: 1.7375e-01 | Actor Loss: -0.3736 | KL: 0.2247 | Time: 13075.2s\n",
      "[215000] Updated reference policy\n",
      "[215000] ===== EVAL: 290.10 =====\n",
      "[216000] Critic Loss1: 2.1303e-01 | Critic Loss2: 2.0523e-01 | Actor Loss: -0.3302 | KL: 0.2619 | Time: 13152.8s\n",
      "[216000] Updated reference policy\n",
      "[217000] Critic Loss1: 3.4780e-01 | Critic Loss2: 3.5032e-01 | Actor Loss: -0.3557 | KL: 0.1960 | Time: 13209.9s\n",
      "[217000] Updated reference policy\n",
      "[218000] Critic Loss1: 1.3700e+00 | Critic Loss2: 1.3945e+00 | Actor Loss: -0.3085 | KL: 0.2158 | Time: 13267.0s\n",
      "[218000] Updated reference policy\n",
      "[219000] Critic Loss1: 4.7664e-01 | Critic Loss2: 5.0211e-01 | Actor Loss: -0.3846 | KL: 0.1766 | Time: 13324.1s\n",
      "[219000] Updated reference policy\n",
      "[220000] Critic Loss1: 5.4472e-01 | Critic Loss2: 5.5638e-01 | Actor Loss: -0.3954 | KL: 0.2000 | Time: 13381.2s\n",
      "[220000] Updated reference policy\n",
      "[220000] ===== EVAL: 270.23 =====\n",
      "[221000] Critic Loss1: 5.7204e-01 | Critic Loss2: 6.0335e-01 | Actor Loss: -0.3137 | KL: 0.1986 | Time: 13458.6s\n",
      "[221000] Updated reference policy\n",
      "[222000] Critic Loss1: 7.9380e-01 | Critic Loss2: 8.0315e-01 | Actor Loss: -0.2925 | KL: 0.2704 | Time: 13515.8s\n",
      "[222000] Updated reference policy\n",
      "[223000] Critic Loss1: 1.1230e+00 | Critic Loss2: 1.2189e+00 | Actor Loss: -0.2753 | KL: 0.2855 | Time: 13572.9s\n",
      "[223000] Updated reference policy\n",
      "Step 223001 | Episode  180 | Reward:   282.89 | Length: 1000 | Time: 13572.9s\n",
      "[224000] Critic Loss1: 6.7220e-01 | Critic Loss2: 6.9979e-01 | Actor Loss: -0.4269 | KL: 0.2793 | Time: 13630.0s\n",
      "[224000] Updated reference policy\n",
      "[225000] Critic Loss1: 4.6557e-01 | Critic Loss2: 3.9853e-01 | Actor Loss: -0.4307 | KL: 0.2405 | Time: 13687.1s\n",
      "[225000] Updated reference policy\n",
      "[225000] ===== EVAL: 278.40 =====\n",
      "[226000] Critic Loss1: 1.9752e-01 | Critic Loss2: 2.3998e-01 | Actor Loss: -0.3617 | KL: 0.2431 | Time: 13764.6s\n",
      "[226000] Updated reference policy\n",
      "[227000] Critic Loss1: 1.5039e+00 | Critic Loss2: 1.5915e+00 | Actor Loss: -0.3394 | KL: 0.2106 | Time: 13821.7s\n",
      "[227000] Updated reference policy\n",
      "[228000] Critic Loss1: 1.0771e+00 | Critic Loss2: 1.0657e+00 | Actor Loss: -0.4274 | KL: 0.2145 | Time: 13878.9s\n",
      "[228000] Updated reference policy\n",
      "[229000] Critic Loss1: 2.1683e+00 | Critic Loss2: 2.1002e+00 | Actor Loss: -0.2979 | KL: 0.2255 | Time: 13936.0s\n",
      "[229000] Updated reference policy\n",
      "[230000] Critic Loss1: 5.6266e+00 | Critic Loss2: 5.4883e+00 | Actor Loss: -0.3479 | KL: 0.2342 | Time: 13993.2s\n",
      "[230000] Updated reference policy\n",
      "[230000] ===== EVAL: 275.75 =====\n",
      "[231000] Critic Loss1: 2.7335e+00 | Critic Loss2: 2.6108e+00 | Actor Loss: -0.3650 | KL: 0.2162 | Time: 14071.3s\n",
      "[231000] Updated reference policy\n",
      "[232000] Critic Loss1: 9.4264e+00 | Critic Loss2: 9.6341e+00 | Actor Loss: -0.2728 | KL: 0.3091 | Time: 14128.6s\n",
      "[232000] Updated reference policy\n",
      "[233000] Critic Loss1: 1.6161e+00 | Critic Loss2: 1.9783e+00 | Actor Loss: -0.4476 | KL: 0.2434 | Time: 14185.8s\n",
      "[233000] Updated reference policy\n",
      "[234000] Critic Loss1: 1.2718e+00 | Critic Loss2: 1.3254e+00 | Actor Loss: -0.4456 | KL: 0.2801 | Time: 14242.9s\n",
      "[234000] Updated reference policy\n",
      "[235000] Critic Loss1: 1.4940e+00 | Critic Loss2: 1.4090e+00 | Actor Loss: -0.4024 | KL: 0.2907 | Time: 14300.1s\n",
      "[235000] Updated reference policy\n",
      "[235000] ===== EVAL: 299.21 =====\n",
      "[236000] Critic Loss1: 1.1527e+00 | Critic Loss2: 1.1141e+00 | Actor Loss: -0.4028 | KL: 0.2406 | Time: 14377.7s\n",
      "[236000] Updated reference policy\n",
      "Step 236001 | Episode  190 | Reward:   587.06 | Length: 2000 | Time: 14377.7s\n",
      "[237000] Critic Loss1: 1.0302e+00 | Critic Loss2: 1.0839e+00 | Actor Loss: -0.1998 | KL: 0.2947 | Time: 14434.7s\n",
      "[237000] Updated reference policy\n",
      "[238000] Critic Loss1: 8.7333e-01 | Critic Loss2: 9.4456e-01 | Actor Loss: -0.4400 | KL: 0.2560 | Time: 14491.8s\n",
      "[238000] Updated reference policy\n",
      "[239000] Critic Loss1: 4.8064e-01 | Critic Loss2: 4.2590e-01 | Actor Loss: -0.4253 | KL: 0.2328 | Time: 14549.0s\n",
      "[239000] Updated reference policy\n",
      "[240000] Critic Loss1: 4.2548e-01 | Critic Loss2: 4.1390e-01 | Actor Loss: -0.3721 | KL: 0.2339 | Time: 14606.1s\n",
      "[240000] Updated reference policy\n",
      "[240000] ===== EVAL: 283.90 =====\n",
      "[241000] Critic Loss1: 7.4806e-01 | Critic Loss2: 7.1026e-01 | Actor Loss: -0.3903 | KL: 0.2407 | Time: 14683.9s\n",
      "[241000] Updated reference policy\n",
      "[242000] Critic Loss1: 7.9871e-01 | Critic Loss2: 7.6023e-01 | Actor Loss: -0.4307 | KL: 0.3243 | Time: 14741.1s\n",
      "[242000] Updated reference policy\n",
      "[243000] Critic Loss1: 1.0239e+00 | Critic Loss2: 8.6028e-01 | Actor Loss: -0.4561 | KL: 0.3004 | Time: 14798.3s\n",
      "[243000] Updated reference policy\n",
      "[244000] Critic Loss1: 1.3357e+00 | Critic Loss2: 9.7302e-01 | Actor Loss: -0.5074 | KL: 0.2883 | Time: 14855.4s\n",
      "[244000] Updated reference policy\n",
      "[245000] Critic Loss1: 1.6797e+00 | Critic Loss2: 1.3113e+00 | Actor Loss: -0.4286 | KL: 0.2474 | Time: 14912.4s\n",
      "[245000] Updated reference policy\n",
      "[245000] ===== EVAL: 298.76 =====\n",
      "[246000] Critic Loss1: 7.3092e-01 | Critic Loss2: 6.9729e-01 | Actor Loss: -0.3858 | KL: 0.2509 | Time: 14989.8s\n",
      "[246000] Updated reference policy\n",
      "[247000] Critic Loss1: 4.7896e+00 | Critic Loss2: 4.0322e+00 | Actor Loss: -0.4096 | KL: 0.2316 | Time: 15046.8s\n",
      "[247000] Updated reference policy\n",
      "[248000] Critic Loss1: 1.9891e+00 | Critic Loss2: 1.7483e+00 | Actor Loss: -0.4449 | KL: 0.2590 | Time: 15103.9s\n",
      "[248000] Updated reference policy\n",
      "Step 248001 | Episode  200 | Reward:   286.78 | Length: 1000 | Time: 15103.9s\n",
      "[249000] Critic Loss1: 6.1203e+00 | Critic Loss2: 5.0044e+00 | Actor Loss: -0.3072 | KL: 0.2390 | Time: 15161.1s\n",
      "[249000] Updated reference policy\n",
      "[250000] Critic Loss1: 1.1243e+01 | Critic Loss2: 1.0576e+01 | Actor Loss: -0.4142 | KL: 0.2253 | Time: 15218.3s\n",
      "[250000] Updated reference policy\n",
      "[250000] ===== EVAL: 276.70 =====\n",
      "[251000] Critic Loss1: 3.2211e+01 | Critic Loss2: 3.2862e+01 | Actor Loss: -0.4663 | KL: 0.2654 | Time: 15296.2s\n",
      "[251000] Updated reference policy\n",
      "[252000] Critic Loss1: 4.4950e+01 | Critic Loss2: 4.4579e+01 | Actor Loss: -0.4499 | KL: 0.2553 | Time: 15353.5s\n",
      "[252000] Updated reference policy\n",
      "[253000] Critic Loss1: 8.8037e+01 | Critic Loss2: 9.3721e+01 | Actor Loss: -0.4215 | KL: 0.2835 | Time: 15410.7s\n",
      "[253000] Updated reference policy\n",
      "[254000] Critic Loss1: 3.6883e+02 | Critic Loss2: 3.9477e+02 | Actor Loss: -0.4300 | KL: 0.2983 | Time: 15468.0s\n",
      "[254000] Updated reference policy\n",
      "[255000] Critic Loss1: 1.3256e+03 | Critic Loss2: 1.4345e+03 | Actor Loss: -0.5276 | KL: 0.4372 | Time: 15525.3s\n",
      "[255000] Updated reference policy\n",
      "[255000] ===== EVAL: 277.59 =====\n",
      "[256000] Critic Loss1: 8.2928e+03 | Critic Loss2: 9.3165e+03 | Actor Loss: -0.6228 | KL: 0.3931 | Time: 15603.2s\n",
      "[256000] Updated reference policy\n",
      "[257000] Critic Loss1: 4.0789e+04 | Critic Loss2: 4.6087e+04 | Actor Loss: -0.5869 | KL: 0.4672 | Time: 15660.3s\n",
      "[257000] Updated reference policy\n",
      "[258000] Critic Loss1: 1.7773e+05 | Critic Loss2: 1.9625e+05 | Actor Loss: -0.6783 | KL: 0.4486 | Time: 15717.3s\n",
      "[258000] Updated reference policy\n",
      "[259000] Critic Loss1: 5.8201e+05 | Critic Loss2: 6.5232e+05 | Actor Loss: -0.6742 | KL: 0.4869 | Time: 15774.4s\n",
      "[259000] Updated reference policy\n",
      "[260000] Critic Loss1: 1.5256e+06 | Critic Loss2: 1.6900e+06 | Actor Loss: -0.6927 | KL: 0.4882 | Time: 15831.4s\n",
      "[260000] Updated reference policy\n",
      "[260000] ===== EVAL: 274.97 =====\n",
      "[261000] Critic Loss1: 4.3644e+06 | Critic Loss2: 4.8775e+06 | Actor Loss: -0.6707 | KL: 0.4285 | Time: 15909.2s\n",
      "[261000] Updated reference policy\n",
      "Step 261001 | Episode  210 | Reward:   555.71 | Length: 2000 | Time: 15909.2s\n",
      "[262000] Critic Loss1: 7.3654e+06 | Critic Loss2: 8.3868e+06 | Actor Loss: -0.6956 | KL: 0.4404 | Time: 15966.2s\n",
      "[262000] Updated reference policy\n",
      "[263000] Critic Loss1: 1.8365e+07 | Critic Loss2: 1.9881e+07 | Actor Loss: -0.7156 | KL: 0.4587 | Time: 16023.3s\n",
      "[263000] Updated reference policy\n",
      "[264000] Critic Loss1: 2.7109e+07 | Critic Loss2: 3.1172e+07 | Actor Loss: -0.6298 | KL: 0.4837 | Time: 16080.5s\n",
      "[264000] Updated reference policy\n",
      "[265000] Critic Loss1: 1.7213e+07 | Critic Loss2: 1.9434e+07 | Actor Loss: -0.6098 | KL: 0.4600 | Time: 16137.6s\n",
      "[265000] Updated reference policy\n",
      "[265000] ===== EVAL: 285.37 =====\n",
      "[266000] Critic Loss1: 2.0994e+07 | Critic Loss2: 2.3670e+07 | Actor Loss: -0.6773 | KL: 0.4841 | Time: 16215.8s\n",
      "[266000] Updated reference policy\n",
      "[267000] Critic Loss1: 3.2184e+07 | Critic Loss2: 3.4824e+07 | Actor Loss: -0.6601 | KL: 0.5007 | Time: 16273.0s\n",
      "[267000] Updated reference policy\n",
      "[268000] Critic Loss1: 3.5861e+07 | Critic Loss2: 3.8691e+07 | Actor Loss: -0.6687 | KL: 0.4516 | Time: 16330.2s\n",
      "[268000] Updated reference policy\n",
      "[269000] Critic Loss1: 2.7559e+07 | Critic Loss2: 3.0389e+07 | Actor Loss: -0.7435 | KL: 0.4509 | Time: 16387.4s\n",
      "[269000] Updated reference policy\n",
      "[270000] Critic Loss1: 2.3420e+07 | Critic Loss2: 2.6257e+07 | Actor Loss: -0.6947 | KL: 0.4600 | Time: 16444.6s\n",
      "[270000] Updated reference policy\n",
      "[270000] ===== EVAL: 305.22 =====\n",
      "[271000] Critic Loss1: 3.2381e+07 | Critic Loss2: 3.5214e+07 | Actor Loss: -0.7138 | KL: 0.4383 | Time: 16522.2s\n",
      "[271000] Updated reference policy\n",
      "[272000] Critic Loss1: 2.7098e+07 | Critic Loss2: 3.0460e+07 | Actor Loss: -0.7205 | KL: 0.4673 | Time: 16579.4s\n",
      "[272000] Updated reference policy\n",
      "[273000] Critic Loss1: 1.8756e+07 | Critic Loss2: 2.0169e+07 | Actor Loss: -0.6579 | KL: 0.4655 | Time: 16636.6s\n",
      "[273000] Updated reference policy\n",
      "Step 273001 | Episode  220 | Reward:   279.20 | Length: 1000 | Time: 16636.6s\n",
      "[274000] Critic Loss1: 2.1131e+07 | Critic Loss2: 2.2107e+07 | Actor Loss: -0.6701 | KL: 0.4385 | Time: 16693.8s\n",
      "[274000] Updated reference policy\n",
      "[275000] Critic Loss1: 2.2442e+07 | Critic Loss2: 2.3231e+07 | Actor Loss: -0.7604 | KL: 0.3996 | Time: 16750.9s\n",
      "[275000] Updated reference policy\n",
      "[275000] ===== EVAL: 273.22 =====\n",
      "[276000] Critic Loss1: 2.5150e+07 | Critic Loss2: 2.4922e+07 | Actor Loss: -0.7159 | KL: 0.4613 | Time: 16828.2s\n",
      "[276000] Updated reference policy\n",
      "[277000] Critic Loss1: 1.7336e+07 | Critic Loss2: 1.7395e+07 | Actor Loss: -0.6202 | KL: 0.5249 | Time: 16885.3s\n",
      "[277000] Updated reference policy\n",
      "[278000] Critic Loss1: 7.7237e+07 | Critic Loss2: 7.9930e+07 | Actor Loss: -0.6720 | KL: 0.4264 | Time: 16942.4s\n",
      "[278000] Updated reference policy\n",
      "[279000] Critic Loss1: 1.6451e+07 | Critic Loss2: 1.7051e+07 | Actor Loss: -0.7138 | KL: 0.4363 | Time: 16999.4s\n",
      "[279000] Updated reference policy\n",
      "[280000] Critic Loss1: 1.3727e+07 | Critic Loss2: 1.3294e+07 | Actor Loss: -0.6742 | KL: 0.4988 | Time: 17056.5s\n",
      "[280000] Updated reference policy\n",
      "[280000] ===== EVAL: 272.23 =====\n",
      "[281000] Critic Loss1: 8.4853e+06 | Critic Loss2: 9.2987e+06 | Actor Loss: -0.6877 | KL: 0.4449 | Time: 17134.1s\n",
      "[281000] Updated reference policy\n",
      "[282000] Critic Loss1: 1.0802e+07 | Critic Loss2: 9.9639e+06 | Actor Loss: -0.7221 | KL: 0.4650 | Time: 17191.1s\n",
      "[282000] Updated reference policy\n",
      "[283000] Critic Loss1: 1.2627e+07 | Critic Loss2: 1.1690e+07 | Actor Loss: -0.6396 | KL: 0.4204 | Time: 17248.2s\n",
      "[283000] Updated reference policy\n",
      "[284000] Critic Loss1: 5.0512e+07 | Critic Loss2: 4.9901e+07 | Actor Loss: -0.7160 | KL: 0.4516 | Time: 17305.2s\n",
      "[284000] Updated reference policy\n",
      "[285000] Critic Loss1: 6.4809e+06 | Critic Loss2: 6.4565e+06 | Actor Loss: -0.7190 | KL: 0.5092 | Time: 17362.3s\n",
      "[285000] Updated reference policy\n",
      "[285000] ===== EVAL: 268.86 =====\n",
      "[286000] Critic Loss1: 6.1547e+06 | Critic Loss2: 6.2206e+06 | Actor Loss: -0.7281 | KL: 0.4048 | Time: 17439.9s\n",
      "[286000] Updated reference policy\n",
      "Step 286001 | Episode  230 | Reward:   539.18 | Length: 2000 | Time: 17439.9s\n",
      "[287000] Critic Loss1: 4.5780e+06 | Critic Loss2: 5.1595e+06 | Actor Loss: -0.6471 | KL: 0.4344 | Time: 17496.9s\n",
      "[287000] Updated reference policy\n",
      "[288000] Critic Loss1: 3.8203e+06 | Critic Loss2: 4.3440e+06 | Actor Loss: -0.6443 | KL: 0.5611 | Time: 17554.1s\n",
      "[288000] Updated reference policy\n",
      "[289000] Critic Loss1: 3.9814e+06 | Critic Loss2: 4.4786e+06 | Actor Loss: -0.6210 | KL: 0.4109 | Time: 17611.2s\n",
      "[289000] Updated reference policy\n",
      "[290000] Critic Loss1: 3.3191e+06 | Critic Loss2: 3.6533e+06 | Actor Loss: -0.7113 | KL: 0.4442 | Time: 17668.4s\n",
      "[290000] Updated reference policy\n",
      "[290000] ===== EVAL: 299.28 =====\n",
      "[291000] Critic Loss1: 3.0807e+06 | Critic Loss2: 3.4466e+06 | Actor Loss: -0.7197 | KL: 0.3883 | Time: 17746.0s\n",
      "[291000] Updated reference policy\n",
      "[292000] Critic Loss1: 5.1250e+06 | Critic Loss2: 5.7930e+06 | Actor Loss: -0.6496 | KL: 0.4288 | Time: 17803.2s\n",
      "[292000] Updated reference policy\n",
      "[293000] Critic Loss1: 2.6453e+06 | Critic Loss2: 3.3243e+06 | Actor Loss: -0.5997 | KL: 0.4361 | Time: 17860.2s\n",
      "[293000] Updated reference policy\n",
      "[294000] Critic Loss1: 4.7932e+06 | Critic Loss2: 5.5862e+06 | Actor Loss: -0.6724 | KL: 0.4270 | Time: 17917.4s\n",
      "[294000] Updated reference policy\n",
      "[295000] Critic Loss1: 5.0468e+06 | Critic Loss2: 5.4739e+06 | Actor Loss: -0.5999 | KL: 0.3607 | Time: 17974.4s\n",
      "[295000] Updated reference policy\n",
      "[295000] ===== EVAL: 290.32 =====\n",
      "[296000] Critic Loss1: 7.0989e+06 | Critic Loss2: 7.9116e+06 | Actor Loss: -0.6276 | KL: 0.4109 | Time: 18052.2s\n",
      "[296000] Updated reference policy\n",
      "[297000] Critic Loss1: 4.7008e+06 | Critic Loss2: 5.2110e+06 | Actor Loss: -0.6188 | KL: 0.4128 | Time: 18109.2s\n",
      "[297000] Updated reference policy\n",
      "[298000] Critic Loss1: 4.4955e+06 | Critic Loss2: 4.6122e+06 | Actor Loss: -0.5867 | KL: 0.4200 | Time: 18166.3s\n",
      "[298000] Updated reference policy\n",
      "Step 298001 | Episode  240 | Reward:   259.17 | Length: 1000 | Time: 18166.3s\n",
      "[299000] Critic Loss1: 3.1418e+06 | Critic Loss2: 3.6840e+06 | Actor Loss: -0.5822 | KL: 0.3646 | Time: 18223.4s\n",
      "[299000] Updated reference policy\n",
      "[300000] Critic Loss1: 6.5527e+06 | Critic Loss2: 6.9874e+06 | Actor Loss: -0.7564 | KL: 0.3975 | Time: 18280.6s\n",
      "[300000] Updated reference policy\n",
      "[300000] ===== EVAL: 298.98 =====\n",
      "[301000] Critic Loss1: 4.4002e+06 | Critic Loss2: 4.7161e+06 | Actor Loss: -0.5975 | KL: 0.3917 | Time: 18358.3s\n",
      "[301000] Updated reference policy\n",
      "[302000] Critic Loss1: 4.4414e+06 | Critic Loss2: 5.0922e+06 | Actor Loss: -0.6296 | KL: 0.3945 | Time: 18415.5s\n",
      "[302000] Updated reference policy\n",
      "[303000] Critic Loss1: 7.8907e+06 | Critic Loss2: 8.1058e+06 | Actor Loss: -0.5581 | KL: 0.4513 | Time: 18472.7s\n",
      "[303000] Updated reference policy\n",
      "[304000] Critic Loss1: 2.5570e+06 | Critic Loss2: 2.7657e+06 | Actor Loss: -0.5596 | KL: 0.3856 | Time: 18529.8s\n",
      "[304000] Updated reference policy\n",
      "[305000] Critic Loss1: 3.0664e+06 | Critic Loss2: 3.2167e+06 | Actor Loss: -0.5635 | KL: 0.4679 | Time: 18586.9s\n",
      "[305000] Updated reference policy\n",
      "[305000] ===== EVAL: 296.04 =====\n",
      "[306000] Critic Loss1: 2.5600e+06 | Critic Loss2: 2.6986e+06 | Actor Loss: -0.6118 | KL: 0.4610 | Time: 18664.7s\n",
      "[306000] Updated reference policy\n",
      "[307000] Critic Loss1: 5.3100e+06 | Critic Loss2: 5.5057e+06 | Actor Loss: -0.6686 | KL: 0.4258 | Time: 18721.9s\n",
      "[307000] Updated reference policy\n",
      "[308000] Critic Loss1: 3.0496e+06 | Critic Loss2: 3.2844e+06 | Actor Loss: -0.6896 | KL: 0.4909 | Time: 18779.1s\n",
      "[308000] Updated reference policy\n",
      "[309000] Critic Loss1: 2.1217e+06 | Critic Loss2: 2.0706e+06 | Actor Loss: -0.5641 | KL: 0.3741 | Time: 18836.3s\n",
      "[309000] Updated reference policy\n",
      "[310000] Critic Loss1: 4.0181e+06 | Critic Loss2: 4.2568e+06 | Actor Loss: -0.6387 | KL: 0.3777 | Time: 18893.4s\n",
      "[310000] Updated reference policy\n",
      "[310000] ===== EVAL: 314.08 =====\n",
      "[311000] Critic Loss1: 3.3736e+06 | Critic Loss2: 3.7151e+06 | Actor Loss: -0.6774 | KL: 0.4059 | Time: 18971.0s\n",
      "[311000] Updated reference policy\n",
      "Step 311001 | Episode  250 | Reward:   565.71 | Length: 2000 | Time: 18971.1s\n",
      "[312000] Critic Loss1: 3.9898e+06 | Critic Loss2: 4.1608e+06 | Actor Loss: -0.6677 | KL: 0.3903 | Time: 19028.3s\n",
      "[312000] Updated reference policy\n",
      "[313000] Critic Loss1: 1.8820e+06 | Critic Loss2: 1.9154e+06 | Actor Loss: -0.7076 | KL: 0.4208 | Time: 19085.5s\n",
      "[313000] Updated reference policy\n",
      "[314000] Critic Loss1: 3.8020e+06 | Critic Loss2: 4.6786e+06 | Actor Loss: -0.7036 | KL: 0.4853 | Time: 19142.7s\n",
      "[314000] Updated reference policy\n",
      "[315000] Critic Loss1: 3.7990e+06 | Critic Loss2: 4.2992e+06 | Actor Loss: -0.6820 | KL: 0.4980 | Time: 19199.9s\n",
      "[315000] Updated reference policy\n",
      "[315000] ===== EVAL: 285.25 =====\n",
      "[316000] Critic Loss1: 2.1814e+06 | Critic Loss2: 2.6003e+06 | Actor Loss: -0.6119 | KL: 0.4261 | Time: 19277.6s\n",
      "[316000] Updated reference policy\n",
      "[317000] Critic Loss1: 9.8652e+05 | Critic Loss2: 1.3381e+06 | Actor Loss: -0.6322 | KL: 0.4472 | Time: 19334.8s\n",
      "[317000] Updated reference policy\n",
      "[318000] Critic Loss1: 5.8406e+06 | Critic Loss2: 7.4473e+06 | Actor Loss: -0.6635 | KL: 0.3915 | Time: 19392.0s\n",
      "[318000] Updated reference policy\n",
      "[319000] Critic Loss1: 8.4912e+06 | Critic Loss2: 8.5188e+06 | Actor Loss: -0.6344 | KL: 0.4890 | Time: 19449.1s\n",
      "[319000] Updated reference policy\n",
      "[320000] Critic Loss1: 1.4684e+06 | Critic Loss2: 1.4411e+06 | Actor Loss: -0.6158 | KL: 0.3737 | Time: 19506.3s\n",
      "[320000] Updated reference policy\n",
      "[320000] ===== EVAL: 383.02 =====\n",
      "[321000] Critic Loss1: 2.9905e+07 | Critic Loss2: 2.9640e+07 | Actor Loss: -0.6596 | KL: 0.3903 | Time: 19583.9s\n",
      "[321000] Updated reference policy\n",
      "[322000] Critic Loss1: 2.1636e+06 | Critic Loss2: 2.2471e+06 | Actor Loss: -0.6721 | KL: 0.4714 | Time: 19641.1s\n",
      "[322000] Updated reference policy\n",
      "[323000] Critic Loss1: 9.1515e+05 | Critic Loss2: 9.2935e+05 | Actor Loss: -0.6495 | KL: 0.4706 | Time: 19698.0s\n",
      "[323000] Updated reference policy\n",
      "Step 323001 | Episode  260 | Reward:   375.71 | Length: 1000 | Time: 19698.0s\n",
      "[324000] Critic Loss1: 5.3905e+06 | Critic Loss2: 6.0333e+06 | Actor Loss: -0.5829 | KL: 0.4342 | Time: 19755.0s\n",
      "[324000] Updated reference policy\n",
      "[325000] Critic Loss1: 5.4206e+06 | Critic Loss2: 5.7415e+06 | Actor Loss: -0.5483 | KL: 0.3672 | Time: 19812.1s\n",
      "[325000] Updated reference policy\n",
      "[325000] ===== EVAL: 331.86 =====\n",
      "[326000] Critic Loss1: 1.0095e+06 | Critic Loss2: 7.6755e+05 | Actor Loss: -0.4565 | KL: 0.3958 | Time: 19889.6s\n",
      "[326000] Updated reference policy\n",
      "[327000] Critic Loss1: 8.8527e+06 | Critic Loss2: 1.0541e+07 | Actor Loss: -0.6866 | KL: 0.4162 | Time: 19946.7s\n",
      "[327000] Updated reference policy\n",
      "[328000] Critic Loss1: 1.8297e+06 | Critic Loss2: 1.5359e+06 | Actor Loss: -0.5717 | KL: 0.3735 | Time: 20003.7s\n",
      "[328000] Updated reference policy\n",
      "[329000] Critic Loss1: 5.4644e+06 | Critic Loss2: 5.3968e+06 | Actor Loss: -0.6407 | KL: 0.3977 | Time: 20060.9s\n",
      "[329000] Updated reference policy\n",
      "[330000] Critic Loss1: 5.7472e+06 | Critic Loss2: 5.0476e+06 | Actor Loss: -0.5523 | KL: 0.3793 | Time: 20118.0s\n",
      "[330000] Updated reference policy\n",
      "[330000] ===== EVAL: 286.72 =====\n",
      "[331000] Critic Loss1: 1.9633e+06 | Critic Loss2: 1.9173e+06 | Actor Loss: -0.6543 | KL: 0.4574 | Time: 20195.6s\n",
      "[331000] Updated reference policy\n",
      "[332000] Critic Loss1: 9.0016e+05 | Critic Loss2: 9.4181e+05 | Actor Loss: -0.6867 | KL: 0.4211 | Time: 20252.6s\n",
      "[332000] Updated reference policy\n",
      "[333000] Critic Loss1: 1.9426e+06 | Critic Loss2: 1.7528e+06 | Actor Loss: -0.5874 | KL: 0.4386 | Time: 20309.7s\n",
      "[333000] Updated reference policy\n",
      "[334000] Critic Loss1: 7.1652e+06 | Critic Loss2: 7.2652e+06 | Actor Loss: -0.6477 | KL: 0.4649 | Time: 20366.8s\n",
      "[334000] Updated reference policy\n",
      "[335000] Critic Loss1: 1.4407e+06 | Critic Loss2: 1.1752e+06 | Actor Loss: -0.6333 | KL: 0.4034 | Time: 20424.0s\n",
      "[335000] Updated reference policy\n",
      "[335000] ===== EVAL: 269.42 =====\n",
      "[336000] Critic Loss1: 2.4876e+06 | Critic Loss2: 3.3682e+06 | Actor Loss: -0.6737 | KL: 0.4372 | Time: 20501.5s\n",
      "[336000] Updated reference policy\n",
      "Step 336001 | Episode  270 | Reward:   532.42 | Length: 2000 | Time: 20501.5s\n",
      "[337000] Critic Loss1: 9.4545e+05 | Critic Loss2: 8.8432e+05 | Actor Loss: -0.6833 | KL: 0.4558 | Time: 20558.7s\n",
      "[337000] Updated reference policy\n",
      "[338000] Critic Loss1: 1.7453e+06 | Critic Loss2: 1.4733e+06 | Actor Loss: -0.6589 | KL: 0.4067 | Time: 20615.8s\n",
      "[338000] Updated reference policy\n",
      "[339000] Critic Loss1: 6.0093e+06 | Critic Loss2: 5.5244e+06 | Actor Loss: -0.6947 | KL: 0.4342 | Time: 20673.1s\n",
      "[339000] Updated reference policy\n",
      "[340000] Critic Loss1: 2.2352e+06 | Critic Loss2: 2.2581e+06 | Actor Loss: -0.6359 | KL: 0.3837 | Time: 20730.3s\n",
      "[340000] Updated reference policy\n",
      "[340000] ===== EVAL: 259.23 =====\n",
      "[341000] Critic Loss1: 8.2156e+06 | Critic Loss2: 8.3335e+06 | Actor Loss: -0.5874 | KL: 0.4581 | Time: 20808.0s\n",
      "[341000] Updated reference policy\n",
      "[342000] Critic Loss1: 1.5696e+06 | Critic Loss2: 1.4537e+06 | Actor Loss: -0.6014 | KL: 0.3882 | Time: 20865.4s\n",
      "[342000] Updated reference policy\n",
      "[343000] Critic Loss1: 1.6042e+06 | Critic Loss2: 9.7727e+05 | Actor Loss: -0.6423 | KL: 0.4509 | Time: 20922.7s\n",
      "[343000] Updated reference policy\n",
      "[344000] Critic Loss1: 2.4406e+06 | Critic Loss2: 2.1383e+06 | Actor Loss: -0.5620 | KL: 0.4108 | Time: 20979.9s\n",
      "[344000] Updated reference policy\n",
      "[345000] Critic Loss1: 9.8042e+06 | Critic Loss2: 9.6127e+06 | Actor Loss: -0.6117 | KL: 0.4418 | Time: 21037.2s\n",
      "[345000] Updated reference policy\n",
      "[345000] ===== EVAL: 253.15 =====\n",
      "[346000] Critic Loss1: 2.5104e+06 | Critic Loss2: 1.6557e+06 | Actor Loss: -0.5908 | KL: 0.4520 | Time: 21115.1s\n",
      "[346000] Updated reference policy\n",
      "[347000] Critic Loss1: 1.2624e+07 | Critic Loss2: 1.2804e+07 | Actor Loss: -0.6688 | KL: 0.4040 | Time: 21172.4s\n",
      "[347000] Updated reference policy\n",
      "[348000] Critic Loss1: 7.5946e+06 | Critic Loss2: 6.8818e+06 | Actor Loss: -0.6088 | KL: 0.4442 | Time: 21229.7s\n",
      "[348000] Updated reference policy\n",
      "Step 348001 | Episode  280 | Reward:   252.98 | Length: 1000 | Time: 21229.7s\n",
      "[349000] Critic Loss1: 4.0192e+06 | Critic Loss2: 3.0936e+06 | Actor Loss: -0.6483 | KL: 0.4074 | Time: 21286.9s\n",
      "[349000] Updated reference policy\n",
      "[350000] Critic Loss1: 3.3653e+06 | Critic Loss2: 2.1977e+06 | Actor Loss: -0.6410 | KL: 0.4272 | Time: 21344.1s\n",
      "[350000] Updated reference policy\n",
      "[350000] ===== EVAL: 250.72 =====\n",
      "[351000] Critic Loss1: 6.4455e+06 | Critic Loss2: 5.4423e+06 | Actor Loss: -0.5940 | KL: 0.4270 | Time: 21421.9s\n",
      "[351000] Updated reference policy\n",
      "[352000] Critic Loss1: 3.0459e+06 | Critic Loss2: 1.6615e+06 | Actor Loss: -0.6391 | KL: 0.4413 | Time: 21479.1s\n",
      "[352000] Updated reference policy\n",
      "[353000] Critic Loss1: 4.5505e+06 | Critic Loss2: 3.5516e+06 | Actor Loss: -0.5666 | KL: 0.3922 | Time: 21536.4s\n",
      "[353000] Updated reference policy\n",
      "[354000] Critic Loss1: 5.8334e+06 | Critic Loss2: 3.4008e+06 | Actor Loss: -0.6880 | KL: 0.4278 | Time: 21593.7s\n",
      "[354000] Updated reference policy\n",
      "[355000] Critic Loss1: 1.0700e+07 | Critic Loss2: 9.6081e+06 | Actor Loss: -0.6026 | KL: 0.4251 | Time: 21651.1s\n",
      "[355000] Updated reference policy\n",
      "[355000] ===== EVAL: 243.02 =====\n",
      "[356000] Critic Loss1: 1.1226e+07 | Critic Loss2: 1.0158e+07 | Actor Loss: -0.5661 | KL: 0.4167 | Time: 21728.9s\n",
      "[356000] Updated reference policy\n",
      "[357000] Critic Loss1: 7.8023e+06 | Critic Loss2: 5.1116e+06 | Actor Loss: -0.5946 | KL: 0.4007 | Time: 21786.3s\n",
      "[357000] Updated reference policy\n",
      "[358000] Critic Loss1: 7.7962e+06 | Critic Loss2: 7.4871e+06 | Actor Loss: -0.6098 | KL: 0.4099 | Time: 21843.5s\n",
      "[358000] Updated reference policy\n",
      "[359000] Critic Loss1: 1.5524e+07 | Critic Loss2: 1.6465e+07 | Actor Loss: -0.6447 | KL: 0.3882 | Time: 21900.7s\n",
      "[359000] Updated reference policy\n",
      "[360000] Critic Loss1: 6.4176e+06 | Critic Loss2: 3.5963e+06 | Actor Loss: -0.6812 | KL: 0.4296 | Time: 21958.0s\n",
      "[360000] Updated reference policy\n",
      "[360000] ===== EVAL: 234.55 =====\n",
      "[361000] Critic Loss1: 6.7845e+06 | Critic Loss2: 4.6680e+06 | Actor Loss: -0.5655 | KL: 0.4316 | Time: 22035.7s\n",
      "[361000] Updated reference policy\n",
      "Step 361001 | Episode  290 | Reward:   468.25 | Length: 2000 | Time: 22035.7s\n",
      "[362000] Critic Loss1: 1.2741e+07 | Critic Loss2: 1.5055e+07 | Actor Loss: -0.6637 | KL: 0.4560 | Time: 22093.4s\n",
      "[362000] Updated reference policy\n",
      "[363000] Critic Loss1: 4.4848e+06 | Critic Loss2: 3.1450e+06 | Actor Loss: -0.5692 | KL: 0.4019 | Time: 22150.9s\n",
      "[363000] Updated reference policy\n",
      "[364000] Critic Loss1: 6.5724e+06 | Critic Loss2: 5.3336e+06 | Actor Loss: -0.5775 | KL: 0.3786 | Time: 22208.5s\n",
      "[364000] Updated reference policy\n",
      "[365000] Critic Loss1: 8.9267e+06 | Critic Loss2: 6.9766e+06 | Actor Loss: -0.5717 | KL: 0.4751 | Time: 22265.7s\n",
      "[365000] Updated reference policy\n",
      "[365000] ===== EVAL: 226.10 =====\n",
      "[366000] Critic Loss1: 3.7518e+07 | Critic Loss2: 4.2237e+07 | Actor Loss: -0.6887 | KL: 0.3867 | Time: 22343.5s\n",
      "[366000] Updated reference policy\n",
      "[367000] Critic Loss1: 8.2994e+06 | Critic Loss2: 9.9878e+06 | Actor Loss: -0.6197 | KL: 0.3573 | Time: 22400.7s\n",
      "[367000] Updated reference policy\n",
      "[368000] Critic Loss1: 2.7467e+07 | Critic Loss2: 3.0940e+07 | Actor Loss: -0.5638 | KL: 0.3981 | Time: 22457.9s\n",
      "[368000] Updated reference policy\n",
      "[369000] Critic Loss1: 1.6363e+07 | Critic Loss2: 1.6645e+07 | Actor Loss: -0.6012 | KL: 0.3611 | Time: 22515.0s\n",
      "[369000] Updated reference policy\n",
      "[370000] Critic Loss1: 1.7081e+07 | Critic Loss2: 1.7810e+07 | Actor Loss: -0.6136 | KL: 0.4205 | Time: 22572.3s\n",
      "[370000] Updated reference policy\n",
      "[370000] ===== EVAL: 219.56 =====\n",
      "[371000] Critic Loss1: 1.7682e+07 | Critic Loss2: 2.0646e+07 | Actor Loss: -0.6789 | KL: 0.4284 | Time: 22649.8s\n",
      "[371000] Updated reference policy\n",
      "[372000] Critic Loss1: 3.9692e+07 | Critic Loss2: 4.4931e+07 | Actor Loss: -0.5935 | KL: 0.4391 | Time: 22706.9s\n",
      "[372000] Updated reference policy\n",
      "[373000] Critic Loss1: 4.1790e+06 | Critic Loss2: 3.8154e+06 | Actor Loss: -0.5822 | KL: 0.4118 | Time: 22763.9s\n",
      "[373000] Updated reference policy\n",
      "Step 373001 | Episode  300 | Reward:   219.69 | Length: 1000 | Time: 22763.9s\n",
      "[374000] Critic Loss1: 1.0514e+07 | Critic Loss2: 9.9938e+06 | Actor Loss: -0.5621 | KL: 0.4181 | Time: 22821.0s\n",
      "[374000] Updated reference policy\n",
      "[375000] Critic Loss1: 3.1020e+07 | Critic Loss2: 3.5874e+07 | Actor Loss: -0.5931 | KL: 0.4752 | Time: 22878.0s\n",
      "[375000] Updated reference policy\n",
      "[375000] ===== EVAL: 222.10 =====\n",
      "[376000] Critic Loss1: 2.7522e+07 | Critic Loss2: 3.4942e+07 | Actor Loss: -0.6211 | KL: 0.4168 | Time: 22955.4s\n",
      "[376000] Updated reference policy\n",
      "[377000] Critic Loss1: 2.3907e+07 | Critic Loss2: 2.3639e+07 | Actor Loss: -0.5718 | KL: 0.3857 | Time: 23012.6s\n",
      "[377000] Updated reference policy\n",
      "[378000] Critic Loss1: 1.1545e+07 | Critic Loss2: 1.1541e+07 | Actor Loss: -0.6612 | KL: 0.3827 | Time: 23069.9s\n",
      "[378000] Updated reference policy\n",
      "[379000] Critic Loss1: 6.9615e+06 | Critic Loss2: 5.4035e+06 | Actor Loss: -0.6348 | KL: 0.4058 | Time: 23127.1s\n",
      "[379000] Updated reference policy\n",
      "[380000] Critic Loss1: 7.4007e+06 | Critic Loss2: 6.8046e+06 | Actor Loss: -0.6586 | KL: 0.4512 | Time: 23184.1s\n",
      "[380000] Updated reference policy\n",
      "[380000] ===== EVAL: 225.81 =====\n",
      "[381000] Critic Loss1: 7.2383e+06 | Critic Loss2: 6.3685e+06 | Actor Loss: -0.5916 | KL: 0.4457 | Time: 23261.9s\n",
      "[381000] Updated reference policy\n",
      "[382000] Critic Loss1: 5.2140e+06 | Critic Loss2: 5.3410e+06 | Actor Loss: -0.6413 | KL: 0.4335 | Time: 23319.2s\n",
      "[382000] Updated reference policy\n",
      "[383000] Critic Loss1: 1.3668e+07 | Critic Loss2: 1.6295e+07 | Actor Loss: -0.6053 | KL: 0.3750 | Time: 23376.4s\n",
      "[383000] Updated reference policy\n",
      "[384000] Critic Loss1: 4.7624e+06 | Critic Loss2: 4.6187e+06 | Actor Loss: -0.6917 | KL: 0.3894 | Time: 23433.7s\n",
      "[384000] Updated reference policy\n",
      "[385000] Critic Loss1: 6.4972e+06 | Critic Loss2: 6.8253e+06 | Actor Loss: -0.6262 | KL: 0.4189 | Time: 23491.0s\n",
      "[385000] Updated reference policy\n",
      "[385000] ===== EVAL: 226.41 =====\n",
      "[386000] Critic Loss1: 2.0010e+07 | Critic Loss2: 2.4381e+07 | Actor Loss: -0.5247 | KL: 0.3940 | Time: 23569.1s\n",
      "[386000] Updated reference policy\n",
      "Step 386001 | Episode  310 | Reward:   455.32 | Length: 2000 | Time: 23569.1s\n",
      "[387000] Critic Loss1: 6.1785e+06 | Critic Loss2: 6.2150e+06 | Actor Loss: -0.5770 | KL: 0.3892 | Time: 23625.9s\n",
      "[387000] Updated reference policy\n",
      "[388000] Critic Loss1: 6.3450e+06 | Critic Loss2: 6.6650e+06 | Actor Loss: -0.6895 | KL: 0.3965 | Time: 23682.7s\n",
      "[388000] Updated reference policy\n",
      "[389000] Critic Loss1: 4.0476e+06 | Critic Loss2: 2.9652e+06 | Actor Loss: -0.6205 | KL: 0.4403 | Time: 23740.0s\n",
      "[389000] Updated reference policy\n",
      "[390000] Critic Loss1: 7.7432e+06 | Critic Loss2: 7.4870e+06 | Actor Loss: -0.6193 | KL: 0.4130 | Time: 23797.2s\n",
      "[390000] Updated reference policy\n",
      "[390000] ===== EVAL: 224.85 =====\n",
      "[391000] Critic Loss1: 3.9533e+07 | Critic Loss2: 4.1498e+07 | Actor Loss: -0.5553 | KL: 0.3984 | Time: 23875.1s\n",
      "[391000] Updated reference policy\n",
      "[392000] Critic Loss1: 4.8502e+07 | Critic Loss2: 5.6059e+07 | Actor Loss: -0.6293 | KL: 0.3720 | Time: 23932.2s\n",
      "[392000] Updated reference policy\n",
      "[393000] Critic Loss1: 7.3796e+06 | Critic Loss2: 6.6880e+06 | Actor Loss: -0.6111 | KL: 0.3957 | Time: 23989.3s\n",
      "[393000] Updated reference policy\n",
      "[394000] Critic Loss1: 4.7834e+06 | Critic Loss2: 4.6570e+06 | Actor Loss: -0.5898 | KL: 0.4517 | Time: 24046.5s\n",
      "[394000] Updated reference policy\n",
      "[395000] Critic Loss1: 4.0570e+07 | Critic Loss2: 4.1283e+07 | Actor Loss: -0.6241 | KL: 0.4413 | Time: 24103.8s\n",
      "[395000] Updated reference policy\n",
      "[395000] ===== EVAL: 227.02 =====\n",
      "[396000] Critic Loss1: 6.3354e+06 | Critic Loss2: 6.2325e+06 | Actor Loss: -0.6201 | KL: 0.3749 | Time: 24181.7s\n",
      "[396000] Updated reference policy\n",
      "[397000] Critic Loss1: 8.3363e+06 | Critic Loss2: 8.2368e+06 | Actor Loss: -0.5939 | KL: 0.4376 | Time: 24239.0s\n",
      "[397000] Updated reference policy\n",
      "[398000] Critic Loss1: 1.0537e+07 | Critic Loss2: 1.3018e+07 | Actor Loss: -0.6227 | KL: 0.4326 | Time: 24296.3s\n",
      "[398000] Updated reference policy\n",
      "Step 398001 | Episode  320 | Reward:   222.87 | Length: 1000 | Time: 24296.3s\n",
      "[399000] Critic Loss1: 8.5890e+06 | Critic Loss2: 7.8733e+06 | Actor Loss: -0.5555 | KL: 0.4339 | Time: 24353.5s\n",
      "[399000] Updated reference policy\n",
      "[400000] Critic Loss1: 2.9335e+07 | Critic Loss2: 2.9469e+07 | Actor Loss: -0.6513 | KL: 0.4290 | Time: 24410.7s\n",
      "[400000] Updated reference policy\n",
      "[400000] ===== EVAL: 239.57 =====\n",
      "[401000] Critic Loss1: 1.2302e+07 | Critic Loss2: 1.4137e+07 | Actor Loss: -0.5723 | KL: 0.3958 | Time: 24488.6s\n",
      "[401000] Updated reference policy\n",
      "[402000] Critic Loss1: 3.5058e+07 | Critic Loss2: 3.5340e+07 | Actor Loss: -0.5748 | KL: 0.4247 | Time: 24546.0s\n",
      "[402000] Updated reference policy\n",
      "[403000] Critic Loss1: 8.4413e+06 | Critic Loss2: 9.7784e+06 | Actor Loss: -0.5518 | KL: 0.3862 | Time: 24603.3s\n",
      "[403000] Updated reference policy\n",
      "[404000] Critic Loss1: 1.4000e+07 | Critic Loss2: 1.5524e+07 | Actor Loss: -0.5551 | KL: 0.3960 | Time: 24660.7s\n",
      "[404000] Updated reference policy\n",
      "[405000] Critic Loss1: 7.8580e+06 | Critic Loss2: 7.1299e+06 | Actor Loss: -0.6119 | KL: 0.4101 | Time: 24717.9s\n",
      "[405000] Updated reference policy\n",
      "[405000] ===== EVAL: 261.41 =====\n",
      "[406000] Critic Loss1: 1.2968e+07 | Critic Loss2: 1.3183e+07 | Actor Loss: -0.4915 | KL: 0.3652 | Time: 24796.0s\n",
      "[406000] Updated reference policy\n",
      "[407000] Critic Loss1: 1.0652e+07 | Critic Loss2: 9.5589e+06 | Actor Loss: -0.6131 | KL: 0.3509 | Time: 24853.2s\n",
      "[407000] Updated reference policy\n",
      "[408000] Critic Loss1: 1.1716e+07 | Critic Loss2: 1.3077e+07 | Actor Loss: -0.5658 | KL: 0.4960 | Time: 24910.6s\n",
      "[408000] Updated reference policy\n",
      "[409000] Critic Loss1: 7.9191e+07 | Critic Loss2: 7.9397e+07 | Actor Loss: -0.6686 | KL: 0.3781 | Time: 24967.7s\n",
      "[409000] Updated reference policy\n",
      "[410000] Critic Loss1: 1.9862e+07 | Critic Loss2: 1.7869e+07 | Actor Loss: -0.7036 | KL: 0.4599 | Time: 25024.8s\n",
      "[410000] Updated reference policy\n",
      "[410000] ===== EVAL: 253.66 =====\n",
      "[411000] Critic Loss1: 1.5592e+07 | Critic Loss2: 1.2217e+07 | Actor Loss: -0.5790 | KL: 0.5683 | Time: 25103.1s\n",
      "[411000] Updated reference policy\n",
      "Step 411001 | Episode  330 | Reward:   520.33 | Length: 2000 | Time: 25103.1s\n",
      "[412000] Critic Loss1: 1.3846e+07 | Critic Loss2: 1.1509e+07 | Actor Loss: -0.6984 | KL: 0.5480 | Time: 25160.4s\n",
      "[412000] Updated reference policy\n",
      "[413000] Critic Loss1: 3.7535e+07 | Critic Loss2: 3.6018e+07 | Actor Loss: -0.6311 | KL: 0.4321 | Time: 25217.7s\n",
      "[413000] Updated reference policy\n",
      "[414000] Critic Loss1: 1.2728e+07 | Critic Loss2: 9.3908e+06 | Actor Loss: -0.6492 | KL: 0.4828 | Time: 25274.8s\n",
      "[414000] Updated reference policy\n",
      "[415000] Critic Loss1: 1.7771e+07 | Critic Loss2: 1.2813e+07 | Actor Loss: -0.6515 | KL: 0.4225 | Time: 25331.9s\n",
      "[415000] Updated reference policy\n",
      "[415000] ===== EVAL: 256.44 =====\n",
      "[416000] Critic Loss1: 1.4895e+07 | Critic Loss2: 1.2737e+07 | Actor Loss: -0.5866 | KL: 0.4241 | Time: 25410.0s\n",
      "[416000] Updated reference policy\n",
      "[417000] Critic Loss1: 1.4868e+07 | Critic Loss2: 1.1482e+07 | Actor Loss: -0.5359 | KL: 0.4214 | Time: 25467.2s\n",
      "[417000] Updated reference policy\n",
      "[418000] Critic Loss1: 1.5001e+07 | Critic Loss2: 1.1142e+07 | Actor Loss: -0.5469 | KL: 0.4019 | Time: 25524.5s\n",
      "[418000] Updated reference policy\n",
      "[419000] Critic Loss1: 1.6080e+07 | Critic Loss2: 1.2392e+07 | Actor Loss: -0.6039 | KL: 0.4793 | Time: 25581.7s\n",
      "[419000] Updated reference policy\n",
      "[420000] Critic Loss1: 2.0205e+07 | Critic Loss2: 1.2668e+07 | Actor Loss: -0.7203 | KL: 0.4288 | Time: 25639.0s\n",
      "[420000] Updated reference policy\n",
      "[420000] ===== EVAL: 288.17 =====\n",
      "[421000] Critic Loss1: 2.1197e+07 | Critic Loss2: 1.3371e+07 | Actor Loss: -0.6815 | KL: 0.5061 | Time: 25716.8s\n",
      "[421000] Updated reference policy\n",
      "[422000] Critic Loss1: 2.9148e+07 | Critic Loss2: 1.9869e+07 | Actor Loss: -0.6809 | KL: 0.5200 | Time: 25774.2s\n",
      "[422000] Updated reference policy\n",
      "[423000] Critic Loss1: 1.6359e+07 | Critic Loss2: 1.1997e+07 | Actor Loss: -0.7311 | KL: 0.4441 | Time: 25831.6s\n",
      "[423000] Updated reference policy\n",
      "Step 423001 | Episode  340 | Reward:   305.71 | Length: 1000 | Time: 25831.6s\n",
      "[424000] Critic Loss1: 1.5293e+07 | Critic Loss2: 1.1354e+07 | Actor Loss: -0.6043 | KL: 0.4247 | Time: 25889.1s\n",
      "[424000] Updated reference policy\n",
      "[425000] Critic Loss1: 1.4944e+07 | Critic Loss2: 1.1924e+07 | Actor Loss: -0.5920 | KL: 0.4126 | Time: 25946.4s\n",
      "[425000] Updated reference policy\n",
      "[425000] ===== EVAL: 301.36 =====\n",
      "[426000] Critic Loss1: 1.8208e+07 | Critic Loss2: 1.2759e+07 | Actor Loss: -0.6607 | KL: 0.4257 | Time: 26024.3s\n",
      "[426000] Updated reference policy\n",
      "[427000] Critic Loss1: 2.0092e+07 | Critic Loss2: 1.5045e+07 | Actor Loss: -0.6586 | KL: 0.5340 | Time: 26081.6s\n",
      "[427000] Updated reference policy\n",
      "[428000] Critic Loss1: 2.2846e+07 | Critic Loss2: 1.4802e+07 | Actor Loss: -0.6598 | KL: 0.4631 | Time: 26139.0s\n",
      "[428000] Updated reference policy\n",
      "[429000] Critic Loss1: 2.7380e+07 | Critic Loss2: 1.7257e+07 | Actor Loss: -0.7365 | KL: 0.4730 | Time: 26196.5s\n",
      "[429000] Updated reference policy\n",
      "[430000] Critic Loss1: 2.1519e+07 | Critic Loss2: 1.3957e+07 | Actor Loss: -0.6558 | KL: 0.4701 | Time: 26253.9s\n",
      "[430000] Updated reference policy\n",
      "[430000] ===== EVAL: 230.47 =====\n",
      "[431000] Critic Loss1: 2.0138e+07 | Critic Loss2: 1.5341e+07 | Actor Loss: -0.6560 | KL: 0.4097 | Time: 26332.5s\n",
      "[431000] Updated reference policy\n",
      "[432000] Critic Loss1: 1.9337e+07 | Critic Loss2: 1.8001e+07 | Actor Loss: -0.7521 | KL: 0.4769 | Time: 26390.1s\n",
      "[432000] Updated reference policy\n",
      "[433000] Critic Loss1: 1.1057e+07 | Critic Loss2: 1.2194e+07 | Actor Loss: -0.6779 | KL: 0.4603 | Time: 26447.8s\n",
      "[433000] Updated reference policy\n",
      "[434000] Critic Loss1: 9.3873e+06 | Critic Loss2: 8.6571e+06 | Actor Loss: -0.6645 | KL: 0.4886 | Time: 26504.9s\n",
      "[434000] Updated reference policy\n",
      "[435000] Critic Loss1: 7.7243e+06 | Critic Loss2: 9.1713e+06 | Actor Loss: -0.6227 | KL: 0.4614 | Time: 26562.4s\n",
      "[435000] Updated reference policy\n",
      "[435000] ===== EVAL: 225.86 =====\n",
      "[436000] Critic Loss1: 9.4032e+06 | Critic Loss2: 1.0896e+07 | Actor Loss: -0.6043 | KL: 0.4307 | Time: 26641.8s\n",
      "[436000] Updated reference policy\n",
      "Step 436001 | Episode  350 | Reward:   451.32 | Length: 2000 | Time: 26641.8s\n",
      "[437000] Critic Loss1: 9.0082e+06 | Critic Loss2: 9.5094e+06 | Actor Loss: -0.5795 | KL: 0.3742 | Time: 26699.5s\n",
      "[437000] Updated reference policy\n",
      "[438000] Critic Loss1: 1.1383e+07 | Critic Loss2: 1.2964e+07 | Actor Loss: -0.6557 | KL: 0.4468 | Time: 26756.8s\n",
      "[438000] Updated reference policy\n",
      "[439000] Critic Loss1: 6.3679e+06 | Critic Loss2: 7.4434e+06 | Actor Loss: -0.6402 | KL: 0.4977 | Time: 26813.9s\n",
      "[439000] Updated reference policy\n",
      "[440000] Critic Loss1: 8.2272e+06 | Critic Loss2: 9.1544e+06 | Actor Loss: -0.6618 | KL: 0.4250 | Time: 26871.1s\n",
      "[440000] Updated reference policy\n",
      "[440000] ===== EVAL: 229.92 =====\n",
      "[441000] Critic Loss1: 5.2668e+06 | Critic Loss2: 7.6250e+06 | Actor Loss: -0.5562 | KL: 0.3981 | Time: 26948.7s\n",
      "[441000] Updated reference policy\n",
      "[442000] Critic Loss1: 6.7008e+06 | Critic Loss2: 6.2225e+06 | Actor Loss: -0.5567 | KL: 0.3662 | Time: 27005.8s\n",
      "[442000] Updated reference policy\n",
      "[443000] Critic Loss1: 4.2568e+06 | Critic Loss2: 5.1271e+06 | Actor Loss: -0.5910 | KL: 0.3677 | Time: 27062.9s\n",
      "[443000] Updated reference policy\n",
      "[444000] Critic Loss1: 5.7524e+06 | Critic Loss2: 6.0195e+06 | Actor Loss: -0.6888 | KL: 0.4451 | Time: 27120.0s\n",
      "[444000] Updated reference policy\n",
      "[445000] Critic Loss1: 6.2557e+06 | Critic Loss2: 7.4923e+06 | Actor Loss: -0.6480 | KL: 0.4606 | Time: 27177.3s\n",
      "[445000] Updated reference policy\n",
      "[445000] ===== EVAL: 232.10 =====\n",
      "[446000] Critic Loss1: 6.0163e+06 | Critic Loss2: 5.4507e+06 | Actor Loss: -0.6245 | KL: 0.4147 | Time: 27252.1s\n",
      "[446000] Updated reference policy\n",
      "[447000] Critic Loss1: 6.6496e+06 | Critic Loss2: 7.8325e+06 | Actor Loss: -0.6579 | KL: 0.4894 | Time: 27308.1s\n",
      "[447000] Updated reference policy\n",
      "[448000] Critic Loss1: 5.4803e+06 | Critic Loss2: 4.3651e+06 | Actor Loss: -0.7173 | KL: 0.4428 | Time: 27365.2s\n",
      "[448000] Updated reference policy\n",
      "Step 448001 | Episode  360 | Reward:   242.73 | Length: 1000 | Time: 27365.2s\n",
      "[449000] Critic Loss1: 8.6359e+06 | Critic Loss2: 9.7532e+06 | Actor Loss: -0.6232 | KL: 0.4664 | Time: 27422.3s\n",
      "[449000] Updated reference policy\n",
      "[450000] Critic Loss1: 4.2281e+06 | Critic Loss2: 4.0673e+06 | Actor Loss: -0.5984 | KL: 0.4220 | Time: 27479.5s\n",
      "[450000] Updated reference policy\n",
      "[450000] ===== EVAL: 249.24 =====\n",
      "[451000] Critic Loss1: 4.6126e+06 | Critic Loss2: 5.2094e+06 | Actor Loss: -0.5760 | KL: 0.4301 | Time: 27557.3s\n",
      "[451000] Updated reference policy\n",
      "[452000] Critic Loss1: 4.3574e+06 | Critic Loss2: 3.8207e+06 | Actor Loss: -0.6247 | KL: 0.4123 | Time: 27614.6s\n",
      "[452000] Updated reference policy\n",
      "[453000] Critic Loss1: 6.0838e+06 | Critic Loss2: 7.7241e+06 | Actor Loss: -0.5823 | KL: 0.4086 | Time: 27671.7s\n",
      "[453000] Updated reference policy\n",
      "[454000] Critic Loss1: 5.8479e+06 | Critic Loss2: 5.5252e+06 | Actor Loss: -0.6090 | KL: 0.3892 | Time: 27728.9s\n",
      "[454000] Updated reference policy\n",
      "[455000] Critic Loss1: 1.9161e+07 | Critic Loss2: 1.9158e+07 | Actor Loss: -0.6178 | KL: 0.4293 | Time: 27786.2s\n",
      "[455000] Updated reference policy\n",
      "[455000] ===== EVAL: 255.83 =====\n",
      "[456000] Critic Loss1: 4.4290e+06 | Critic Loss2: 4.2466e+06 | Actor Loss: -0.6291 | KL: 0.4571 | Time: 27864.4s\n",
      "[456000] Updated reference policy\n",
      "[457000] Critic Loss1: 3.5839e+06 | Critic Loss2: 3.8793e+06 | Actor Loss: -0.6051 | KL: 0.4183 | Time: 27921.7s\n",
      "[457000] Updated reference policy\n",
      "[458000] Critic Loss1: 9.1899e+06 | Critic Loss2: 8.9282e+06 | Actor Loss: -0.5514 | KL: 0.3898 | Time: 27979.0s\n",
      "[458000] Updated reference policy\n",
      "[459000] Critic Loss1: 3.8391e+06 | Critic Loss2: 4.3278e+06 | Actor Loss: -0.5020 | KL: 0.3304 | Time: 28036.3s\n",
      "[459000] Updated reference policy\n",
      "[460000] Critic Loss1: 1.7662e+07 | Critic Loss2: 1.7714e+07 | Actor Loss: -0.5233 | KL: 0.3809 | Time: 28093.6s\n",
      "[460000] Updated reference policy\n",
      "[460000] ===== EVAL: 255.27 =====\n",
      "[461000] Critic Loss1: 3.8571e+06 | Critic Loss2: 3.4604e+06 | Actor Loss: -0.6183 | KL: 0.3668 | Time: 28171.7s\n",
      "[461000] Updated reference policy\n",
      "Step 461001 | Episode  370 | Reward:   514.76 | Length: 2000 | Time: 28171.7s\n",
      "[462000] Critic Loss1: 3.7080e+06 | Critic Loss2: 4.1856e+06 | Actor Loss: -0.5340 | KL: 0.4028 | Time: 28228.9s\n",
      "[462000] Updated reference policy\n",
      "[463000] Critic Loss1: 2.6407e+06 | Critic Loss2: 3.2255e+06 | Actor Loss: -0.5375 | KL: 0.3983 | Time: 28286.2s\n",
      "[463000] Updated reference policy\n",
      "[464000] Critic Loss1: 4.0727e+06 | Critic Loss2: 4.2412e+06 | Actor Loss: -0.6187 | KL: 0.3938 | Time: 28343.7s\n",
      "[464000] Updated reference policy\n",
      "[465000] Critic Loss1: 3.5562e+06 | Critic Loss2: 3.5832e+06 | Actor Loss: -0.5650 | KL: 0.3575 | Time: 28401.1s\n",
      "[465000] Updated reference policy\n",
      "[465000] ===== EVAL: 271.02 =====\n",
      "[466000] Critic Loss1: 2.2040e+06 | Critic Loss2: 3.5870e+06 | Actor Loss: -0.5981 | KL: 0.3758 | Time: 28478.7s\n",
      "[466000] Updated reference policy\n",
      "[467000] Critic Loss1: 3.0795e+06 | Critic Loss2: 3.2005e+06 | Actor Loss: -0.6046 | KL: 0.4230 | Time: 28535.8s\n",
      "[467000] Updated reference policy\n",
      "[468000] Critic Loss1: 2.1491e+07 | Critic Loss2: 2.2661e+07 | Actor Loss: -0.5877 | KL: 0.3876 | Time: 28592.9s\n",
      "[468000] Updated reference policy\n",
      "[469000] Critic Loss1: 9.4219e+06 | Critic Loss2: 8.0490e+06 | Actor Loss: -0.5585 | KL: 0.4126 | Time: 28650.1s\n",
      "[469000] Updated reference policy\n",
      "[470000] Critic Loss1: 8.5259e+06 | Critic Loss2: 8.8846e+06 | Actor Loss: -0.5287 | KL: 0.3487 | Time: 28707.1s\n",
      "[470000] Updated reference policy\n",
      "[470000] ===== EVAL: 263.20 =====\n",
      "[471000] Critic Loss1: 3.4322e+06 | Critic Loss2: 3.1506e+06 | Actor Loss: -0.5690 | KL: 0.3312 | Time: 28784.5s\n",
      "[471000] Updated reference policy\n",
      "[472000] Critic Loss1: 3.7319e+06 | Critic Loss2: 3.2069e+06 | Actor Loss: -0.5842 | KL: 0.3632 | Time: 28841.7s\n",
      "[472000] Updated reference policy\n",
      "[473000] Critic Loss1: 8.7429e+06 | Critic Loss2: 8.9737e+06 | Actor Loss: -0.6208 | KL: 0.4307 | Time: 28898.7s\n",
      "[473000] Updated reference policy\n",
      "Step 473001 | Episode  380 | Reward:   282.83 | Length: 1000 | Time: 28898.7s\n",
      "[474000] Critic Loss1: 8.2271e+06 | Critic Loss2: 7.9805e+06 | Actor Loss: -0.6395 | KL: 0.4788 | Time: 28955.8s\n",
      "[474000] Updated reference policy\n",
      "[475000] Critic Loss1: 7.3068e+06 | Critic Loss2: 8.3861e+06 | Actor Loss: -0.6239 | KL: 0.4555 | Time: 29012.9s\n",
      "[475000] Updated reference policy\n",
      "[475000] ===== EVAL: 287.11 =====\n",
      "[476000] Critic Loss1: 5.4690e+06 | Critic Loss2: 4.6831e+06 | Actor Loss: -0.7582 | KL: 0.4425 | Time: 29090.5s\n",
      "[476000] Updated reference policy\n",
      "[477000] Critic Loss1: 6.8744e+06 | Critic Loss2: 5.5316e+06 | Actor Loss: -0.6879 | KL: 0.4347 | Time: 29147.8s\n",
      "[477000] Updated reference policy\n",
      "[478000] Critic Loss1: 4.1500e+06 | Critic Loss2: 3.4500e+06 | Actor Loss: -0.6226 | KL: 0.3993 | Time: 29205.4s\n",
      "[478000] Updated reference policy\n",
      "[479000] Critic Loss1: 4.0032e+06 | Critic Loss2: 4.0932e+06 | Actor Loss: -0.6117 | KL: 0.3558 | Time: 29263.2s\n",
      "[479000] Updated reference policy\n",
      "[480000] Critic Loss1: 4.1287e+06 | Critic Loss2: 3.7049e+06 | Actor Loss: -0.5583 | KL: 0.4047 | Time: 29321.1s\n",
      "[480000] Updated reference policy\n",
      "[480000] ===== EVAL: 289.85 =====\n",
      "[481000] Critic Loss1: 3.2275e+06 | Critic Loss2: 2.7726e+06 | Actor Loss: -0.6110 | KL: 0.3647 | Time: 29400.3s\n",
      "[481000] Updated reference policy\n",
      "[482000] Critic Loss1: 2.9259e+06 | Critic Loss2: 2.6508e+06 | Actor Loss: -0.5114 | KL: 0.3341 | Time: 29458.1s\n",
      "[482000] Updated reference policy\n",
      "[483000] Critic Loss1: 3.6520e+06 | Critic Loss2: 2.5869e+06 | Actor Loss: -0.5844 | KL: 0.3520 | Time: 29515.9s\n",
      "[483000] Updated reference policy\n",
      "[484000] Critic Loss1: 2.9858e+06 | Critic Loss2: 2.8852e+06 | Actor Loss: -0.5134 | KL: 0.3666 | Time: 29573.5s\n",
      "[484000] Updated reference policy\n",
      "[485000] Critic Loss1: 2.4982e+06 | Critic Loss2: 2.1217e+06 | Actor Loss: -0.5289 | KL: 0.3699 | Time: 29631.4s\n",
      "[485000] Updated reference policy\n",
      "[485000] ===== EVAL: 281.73 =====\n",
      "[486000] Critic Loss1: 2.0729e+06 | Critic Loss2: 1.8510e+06 | Actor Loss: -0.5951 | KL: 0.3624 | Time: 29710.2s\n",
      "[486000] Updated reference policy\n",
      "Step 486001 | Episode  390 | Reward:   565.33 | Length: 2000 | Time: 29710.2s\n",
      "[487000] Critic Loss1: 5.4904e+06 | Critic Loss2: 4.4725e+06 | Actor Loss: -0.4791 | KL: 0.3941 | Time: 29767.3s\n",
      "[487000] Updated reference policy\n",
      "[488000] Critic Loss1: 2.5676e+06 | Critic Loss2: 2.0919e+06 | Actor Loss: -0.4830 | KL: 0.3345 | Time: 29821.3s\n",
      "[488000] Updated reference policy\n",
      "[489000] Critic Loss1: 3.4418e+06 | Critic Loss2: 3.3253e+06 | Actor Loss: -0.4858 | KL: 0.3203 | Time: 29875.4s\n",
      "[489000] Updated reference policy\n",
      "[490000] Critic Loss1: 3.1806e+06 | Critic Loss2: 2.0392e+06 | Actor Loss: -0.4768 | KL: 0.3295 | Time: 29929.4s\n",
      "[490000] Updated reference policy\n",
      "[490000] ===== EVAL: 275.06 =====\n",
      "[491000] Critic Loss1: 2.1959e+06 | Critic Loss2: 1.7872e+06 | Actor Loss: -0.5094 | KL: 0.3621 | Time: 30005.1s\n",
      "[491000] Updated reference policy\n",
      "[492000] Critic Loss1: 2.1491e+06 | Critic Loss2: 1.7670e+06 | Actor Loss: -0.5796 | KL: 0.3802 | Time: 30060.8s\n",
      "[492000] Updated reference policy\n",
      "[493000] Critic Loss1: 3.4044e+06 | Critic Loss2: 2.0671e+06 | Actor Loss: -0.5017 | KL: 0.4747 | Time: 30114.8s\n",
      "[493000] Updated reference policy\n",
      "[494000] Critic Loss1: 6.1323e+06 | Critic Loss2: 4.3416e+06 | Actor Loss: -0.5792 | KL: 0.3763 | Time: 30168.9s\n",
      "[494000] Updated reference policy\n",
      "[495000] Critic Loss1: 6.3327e+06 | Critic Loss2: 5.5119e+06 | Actor Loss: -0.6158 | KL: 0.3844 | Time: 30224.3s\n",
      "[495000] Updated reference policy\n",
      "[495000] ===== EVAL: 258.50 =====\n",
      "[496000] Critic Loss1: 1.8209e+06 | Critic Loss2: 1.3495e+06 | Actor Loss: -0.5621 | KL: 0.3495 | Time: 30301.8s\n",
      "[496000] Updated reference policy\n",
      "[497000] Critic Loss1: 1.9400e+06 | Critic Loss2: 1.9066e+06 | Actor Loss: -0.5272 | KL: 0.3068 | Time: 30359.0s\n",
      "[497000] Updated reference policy\n",
      "[498000] Critic Loss1: 2.0722e+06 | Critic Loss2: 1.6881e+06 | Actor Loss: -0.4122 | KL: 0.2966 | Time: 30416.1s\n",
      "[498000] Updated reference policy\n",
      "Step 498001 | Episode  400 | Reward:   271.32 | Length: 1000 | Time: 30416.2s\n",
      "[499000] Critic Loss1: 2.3538e+06 | Critic Loss2: 1.7865e+06 | Actor Loss: -0.3540 | KL: 0.2838 | Time: 30473.4s\n",
      "[499000] Updated reference policy\n",
      "[500000] Critic Loss1: 4.7857e+06 | Critic Loss2: 2.8299e+06 | Actor Loss: -0.4799 | KL: 0.3714 | Time: 30530.9s\n",
      "[500000] Updated reference policy\n",
      "[500000] ===== EVAL: 276.80 =====\n",
      "\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dm_control import suite\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ---------------------- Utilities ----------------------\n",
    "class RunningMeanStd:\n",
    "    \"\"\"Online running mean/std for observation normalization.\"\"\"\n",
    "    def __init__(self, shape, eps=1e-4, clip=10.0):\n",
    "        self.mean = np.zeros(shape, dtype=np.float64)\n",
    "        self.var = np.ones(shape, dtype=np.float64)\n",
    "        self.count = eps\n",
    "        self.clip = clip\n",
    "\n",
    "    def update(self, x: np.ndarray):\n",
    "        # x is (N, dim) or (dim,)\n",
    "        x = np.asarray(x)\n",
    "        if x.ndim == 1:\n",
    "            x = x[None, :]\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_var = np.var(x, axis=0)\n",
    "        batch_count = x.shape[0]\n",
    "        self._update_from_moments(batch_mean, batch_var, batch_count)\n",
    "\n",
    "    def _update_from_moments(self, batch_mean, batch_var, batch_count):\n",
    "        # Welford-like update\n",
    "        delta = batch_mean - self.mean\n",
    "        tot_count = self.count + batch_count\n",
    "\n",
    "        new_mean = self.mean + delta * batch_count / tot_count\n",
    "        m_a = self.var * (self.count)\n",
    "        m_b = batch_var * (batch_count)\n",
    "        M2 = m_a + m_b + (delta ** 2) * self.count * batch_count / tot_count\n",
    "        new_var = M2 / (tot_count)\n",
    "\n",
    "        self.mean = new_mean\n",
    "        self.var = new_var\n",
    "        self.count = tot_count\n",
    "\n",
    "    def normalize(self, x: np.ndarray):\n",
    "        return np.clip((x - self.mean) / (np.sqrt(self.var) + 1e-8), -self.clip, self.clip)\n",
    "\n",
    "def flatten_obs(obs):\n",
    "    \"\"\"Flatten observation dict to single array.\"\"\"\n",
    "    if isinstance(obs, dict):\n",
    "        parts = [np.asarray(v).ravel() for v in obs.values()]\n",
    "        return np.concatenate(parts).astype(np.float32)\n",
    "    else:\n",
    "        return np.asarray(obs).astype(np.float32).ravel()\n",
    "\n",
    "# ---------------------- Replay Buffer ----------------------\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Efficient replay buffer with numpy storage.\"\"\"\n",
    "    def __init__(self, size=200000):\n",
    "        self.max_size = size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        \n",
    "        # Pre-allocate arrays (filled on first push)\n",
    "        self.states = None\n",
    "        self.actions = None\n",
    "        self.rewards = None\n",
    "        self.next_states = None\n",
    "        self.dones = None\n",
    "\n",
    "    def push(self, s, a, r, s2, d):\n",
    "        \"\"\"Store transition.\"\"\"\n",
    "        # Initialize arrays on first push\n",
    "        if self.states is None:\n",
    "            self.states = np.zeros((self.max_size, len(s)), dtype=np.float32)\n",
    "            self.actions = np.zeros((self.max_size, len(a)), dtype=np.float32)\n",
    "            self.rewards = np.zeros(self.max_size, dtype=np.float32)\n",
    "            self.next_states = np.zeros((self.max_size, len(s2)), dtype=np.float32)\n",
    "            self.dones = np.zeros(self.max_size, dtype=np.float32)\n",
    "        \n",
    "        self.states[self.ptr] = s\n",
    "        self.actions[self.ptr] = a\n",
    "        self.rewards[self.ptr] = r\n",
    "        self.next_states[self.ptr] = s2\n",
    "        self.dones[self.ptr] = d\n",
    "        \n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample batch and convert to GPU tensors once.\"\"\"\n",
    "        idx = np.random.randint(0, self.size, size=batch_size)\n",
    "        \n",
    "        return (\n",
    "            torch.from_numpy(self.states[idx]).to(device),\n",
    "            torch.from_numpy(self.actions[idx]).to(device),\n",
    "            torch.from_numpy(self.rewards[idx]).to(device).unsqueeze(-1),\n",
    "            torch.from_numpy(self.next_states[idx]).to(device),\n",
    "            torch.from_numpy(self.dones[idx]).to(device).unsqueeze(-1)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "# ---------------------- Networks ----------------------\n",
    "def orthogonal_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "        nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Gaussian policy network.\"\"\"\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=256, log_std_min=-5, log_std_max=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.mu = nn.Linear(hidden_dim, act_dim)\n",
    "        self.log_std = nn.Linear(hidden_dim, act_dim)\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        self.apply(orthogonal_init)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        h = self.net(obs)\n",
    "        mu = self.mu(h)\n",
    "        log_std = torch.tanh(self.log_std(h))  # bound to (-1,1)\n",
    "        # scale to range\n",
    "        log_std = self.log_std_min + 0.5 * (log_std + 1.0) * (self.log_std_max - self.log_std_min)\n",
    "        std = torch.exp(log_std)\n",
    "        return mu, std, log_std\n",
    "\n",
    "    def sample(self, obs):\n",
    "        mu, std, log_std = self.forward(obs)\n",
    "        eps = torch.randn_like(mu)\n",
    "        a = mu + eps * std\n",
    "        var = std ** 2\n",
    "        log_prob = -0.5 * (((a - mu) ** 2) / var + 2 * log_std + math.log(2 * math.pi))\n",
    "        log_prob = log_prob.sum(-1, keepdim=True)\n",
    "        return a, log_prob\n",
    "\n",
    "    def log_prob(self, obs, a):\n",
    "        mu, std, log_std = self.forward(obs)\n",
    "        var = std ** 2\n",
    "        log_prob = -0.5 * (((a - mu) ** 2) / var + 2 * log_std + math.log(2 * math.pi))\n",
    "        return log_prob.sum(-1, keepdim=True)\n",
    "\n",
    "    def kl(self, obs, other_mu, other_std):\n",
    "        mu, std, _ = self.forward(obs)\n",
    "        var = std ** 2\n",
    "        other_var = other_std ** 2\n",
    "        kl_div = (torch.log(std / other_std) + (other_var + (other_mu - mu) ** 2) / (2 * var) - 0.5)\n",
    "        return kl_div.sum(-1, keepdim=True)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Q-function network.\"\"\"\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim, hidden_dim), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        self.apply(orthogonal_init)\n",
    "    \n",
    "    def forward(self, s, a):\n",
    "        return self.net(torch.cat([s, a], dim=-1))\n",
    "\n",
    "# ---------------------- PMPO Trainer (stabilized) ----------------------\n",
    "class PMPO:\n",
    "    \"\"\"Preference Matching Policy Optimization with twin critics and stability fixes.\"\"\"\n",
    "    def __init__(self, obs_dim, act_dim, \n",
    "                 actor_lr=1e-5, critic_lr=1e-4,\n",
    "                 gamma=0.99, tau=0.995,\n",
    "                 alpha=0.5, beta=1.5,\n",
    "                 grad_clip=1.0,\n",
    "                 target_update_freq=250,\n",
    "                 critic_warmup_steps=5000,\n",
    "                 actor_update_every=2,\n",
    "                 q_clip=1e6):\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau  # used only for soft updates if desired\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.grad_clip = grad_clip\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.critic_warmup_steps = critic_warmup_steps\n",
    "        self.actor_update_every = actor_update_every\n",
    "        self.q_clip = q_clip\n",
    "\n",
    "        # Networks: actor + reference actor\n",
    "        self.actor = Actor(obs_dim, act_dim).to(device)\n",
    "        self.ref_actor = Actor(obs_dim, act_dim).to(device)\n",
    "        self.ref_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.ref_actor.eval()\n",
    "\n",
    "        # Twin critics (Q1, Q2) and twin targets\n",
    "        self.critic1 = Critic(obs_dim, act_dim).to(device)\n",
    "        self.critic2 = Critic(obs_dim, act_dim).to(device)\n",
    "        self.target_critic1 = Critic(obs_dim, act_dim).to(device)\n",
    "        self.target_critic2 = Critic(obs_dim, act_dim).to(device)\n",
    "        self.target_critic1.load_state_dict(self.critic1.state_dict())\n",
    "        self.target_critic2.load_state_dict(self.critic2.state_dict())\n",
    "\n",
    "        # Optimizers\n",
    "        self.actor_opt = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic1_opt = torch.optim.Adam(self.critic1.parameters(), lr=critic_lr)\n",
    "        self.critic2_opt = torch.optim.Adam(self.critic2.parameters(), lr=critic_lr)\n",
    "\n",
    "        # Counters\n",
    "        self._total_steps = 0\n",
    "        self._critic_updates = 0\n",
    "\n",
    "    def update_critic(self, batch):\n",
    "        \"\"\"Update twin critics with min-target to avoid overestimation.\"\"\"\n",
    "        s, a, r, s2, d = batch\n",
    "\n",
    "        # clamp rewards moderately (optional) to keep scale manageable\n",
    "        r = torch.clamp(r, -1e3, 1e3)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # next actions from current actor\n",
    "            a2, _ = self.actor.sample(s2)\n",
    "            q1_next = self.target_critic1(s2, a2)\n",
    "            q2_next = self.target_critic2(s2, a2)\n",
    "            q_next = torch.min(q1_next, q2_next)\n",
    "\n",
    "            y = r + self.gamma * (1.0 - d) * q_next\n",
    "            y = torch.clamp(y, -self.q_clip, self.q_clip)\n",
    "\n",
    "        # Critic 1\n",
    "        q1 = self.critic1(s, a)\n",
    "        loss1 = F.mse_loss(q1, y)\n",
    "        self.critic1_opt.zero_grad()\n",
    "        loss1.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic1.parameters(), self.grad_clip)\n",
    "        self.critic1_opt.step()\n",
    "\n",
    "        # Critic 2\n",
    "        q2 = self.critic2(s, a)\n",
    "        loss2 = F.mse_loss(q2, y)\n",
    "        self.critic2_opt.zero_grad()\n",
    "        loss2.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic2.parameters(), self.grad_clip)\n",
    "        self.critic2_opt.step()\n",
    "\n",
    "        # Hard target sync periodically (more stable than immediate polyak if tuned)\n",
    "        self._critic_updates += 1\n",
    "        if (self._critic_updates % self.target_update_freq) == 0:\n",
    "            self.target_critic1.load_state_dict(self.critic1.state_dict())\n",
    "            self.target_critic2.load_state_dict(self.critic2.state_dict())\n",
    "\n",
    "        return (loss1.item(), loss2.item())\n",
    "\n",
    "    def update_actor(self, s, K=16):\n",
    "        \"\"\"Update policy with preference-based objective (PMPO).\"\"\"\n",
    "        B = s.size(0)\n",
    "\n",
    "        # Sample K actions per state from reference policy (stay on device)\n",
    "        s_rep = s[:, None, :].expand(B, K, -1).reshape(B * K, -1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            mu_ref, std_ref, _ = self.ref_actor.forward(s_rep)\n",
    "            eps = torch.randn_like(mu_ref)\n",
    "            acts = mu_ref + eps * std_ref\n",
    "            qs = self.critic1(s_rep, acts).reshape(B, K)  # could use avg of critics, using critic1 for ranking is fine\n",
    "\n",
    "        # Rank actions\n",
    "        idx = torch.argsort(qs, dim=1)  # ascending\n",
    "        top2_idx = idx[:, -2:]  # highest Q-values\n",
    "        bot2_idx = idx[:, :2]   # lowest Q-values\n",
    "\n",
    "        # Advanced indexing to gather actions/states (no Python loops)\n",
    "        batch_idx = torch.arange(B, device=device)[:, None].expand(B, 2)\n",
    "        top_flat = (batch_idx * K + top2_idx).reshape(-1)\n",
    "        bot_flat = (batch_idx * K + bot2_idx).reshape(-1)\n",
    "\n",
    "        pref_s = s.repeat_interleave(2, dim=0)\n",
    "        pref_a = acts[top_flat]\n",
    "        rej_s = s.repeat_interleave(2, dim=0)\n",
    "        rej_a = acts[bot_flat]\n",
    "\n",
    "        # Compute PMPO terms\n",
    "        logp_pref = self.actor.log_prob(pref_s, pref_a).mean()\n",
    "        logp_rej = self.actor.log_prob(rej_s, rej_a).mean()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mu_r, std_r, _ = self.ref_actor.forward(s)\n",
    "        kl = self.actor.kl(s, mu_r, std_r).mean()\n",
    "\n",
    "        # PMPO objective: maximize J -> minimize -J\n",
    "        J = self.alpha * logp_pref - (1.0 - self.alpha) * logp_rej - self.beta * kl\n",
    "        loss = -J\n",
    "\n",
    "        self.actor_opt.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.grad_clip)\n",
    "        self.actor_opt.step()\n",
    "\n",
    "        return loss.item(), kl.item()\n",
    "\n",
    "    def update_reference(self):\n",
    "        \"\"\"Update reference policy to current policy.\"\"\"\n",
    "        self.ref_actor.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, obs, deterministic=False):\n",
    "        \"\"\"Select action (for evaluation).\"\"\"\n",
    "        obs_t = torch.from_numpy(obs).float().to(device).unsqueeze(0)\n",
    "        if deterministic:\n",
    "            mu, _, _ = self.actor.forward(obs_t)\n",
    "            return mu.cpu().numpy()[0]\n",
    "        else:\n",
    "            a, _ = self.actor.sample(obs_t)\n",
    "            return a.cpu().numpy()[0]\n",
    "\n",
    "# ---------------------- Training Loop ----------------------\n",
    "def train(domain=\"cheetah\", task=\"run\", \n",
    "          total_steps=500000,\n",
    "          batch_size=256,\n",
    "          start_steps=1000,\n",
    "          ref_update_freq=1000,\n",
    "          eval_freq=5000,\n",
    "          eval_episodes=5,\n",
    "          seed=0):\n",
    "    \"\"\"\n",
    "    Main training loop with stabilization changes.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    env = suite.load(domain, task)\n",
    "    ts = env.reset()\n",
    "    s = flatten_obs(ts.observation)\n",
    "\n",
    "    # obs normalization\n",
    "    obs_rms = RunningMeanStd(len(s))\n",
    "\n",
    "    obs_dim = s.size\n",
    "    act_dim = env.action_spec().shape[0]\n",
    "    \n",
    "    print(f\"Environment: {domain}-{task}\")\n",
    "    print(f\"Obs dim: {obs_dim}, Act dim: {act_dim}\")\n",
    "    \n",
    "    # Initialize\n",
    "    rb = ReplayBuffer(size=200000)\n",
    "    algo = PMPO(obs_dim, act_dim,\n",
    "                actor_lr=1e-5,    # smaller actor lr\n",
    "                critic_lr=1e-4,   # smaller critic lr\n",
    "                gamma=0.99,\n",
    "                tau=0.995,\n",
    "                alpha=0.5,\n",
    "                beta=1.5,\n",
    "                grad_clip=1.0,\n",
    "                target_update_freq=250,\n",
    "                critic_warmup_steps=5000,\n",
    "                actor_update_every=2,\n",
    "                q_clip=1e5)\n",
    "\n",
    "    # Action bounds\n",
    "    act_low = env.action_spec().minimum\n",
    "    act_high = env.action_spec().maximum\n",
    "    \n",
    "    # Metrics\n",
    "    episode_reward = 0.0\n",
    "    episode_step = 0\n",
    "    episode_count = 0\n",
    "    start_time = time.time()\n",
    "    last_eval = 0\n",
    "\n",
    "    for t in range(1, total_steps + 1):\n",
    "        # Select action\n",
    "        if len(rb) < start_steps:\n",
    "            a = np.random.uniform(act_low, act_high, size=act_dim).astype(np.float32)\n",
    "        else:\n",
    "            # normalize obs\n",
    "            obs_norm = obs_rms.normalize(s)\n",
    "            a = algo.act(obs_norm, deterministic=False)\n",
    "            a = np.clip(a, act_low, act_high)\n",
    "\n",
    "        # Environment step\n",
    "        ts = env.step(a)\n",
    "        s2 = flatten_obs(ts.observation)\n",
    "        k = ts.reward\n",
    "        if k is None:\n",
    "            k = 0.0\n",
    "        r = float(k)\n",
    "        d = float(ts.last())\n",
    "        \n",
    "        episode_reward += r\n",
    "        episode_step += 1\n",
    "        \n",
    "        # Store transition (store raw obs, normalize when sampling/training)\n",
    "        rb.push(s, a, r, s2, d)\n",
    "\n",
    "        # Update running obs stats (do it on CPU numpy)\n",
    "        obs_rms.update(s2)\n",
    "\n",
    "        # Reset handling\n",
    "        if d:\n",
    "            episode_count += 1\n",
    "            if episode_count % 10 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"Step {t:6d} | Episode {episode_count:4d} | \"\n",
    "                      f\"Reward: {episode_reward:8.2f} | Length: {episode_step:4d} | Time: {elapsed:.1f}s\")\n",
    "            \n",
    "            ts = env.reset()\n",
    "            s = flatten_obs(ts.observation)\n",
    "            episode_reward = 0.0\n",
    "            episode_step = 0\n",
    "        else:\n",
    "            s = s2\n",
    "\n",
    "        # Training updates\n",
    "        if len(rb) >= start_steps:\n",
    "            batch = rb.sample(batch_size)\n",
    "\n",
    "            # BEFORE feeding to networks, normalize states and next_states in-tensor\n",
    "            bs, ba, br, bs2, bd = batch\n",
    "            # Convert to numpy to normalize with obs_rms then back (we avoid CPU-GPU ping-pong by normalizing on CPU once per batch)\n",
    "            bs_np = bs.cpu().numpy()\n",
    "            bs2_np = bs2.cpu().numpy()\n",
    "            bs_norm = torch.from_numpy(np.clip((bs_np - obs_rms.mean) / (np.sqrt(obs_rms.var) + 1e-8), -obs_rms.clip, obs_rms.clip)).float().to(device)\n",
    "            bs2_norm = torch.from_numpy(np.clip((bs2_np - obs_rms.mean) / (np.sqrt(obs_rms.var) + 1e-8), -obs_rms.clip, obs_rms.clip)).float().to(device)\n",
    "\n",
    "            # Replace states in batch with normalized tensors for training\n",
    "            train_batch = (bs_norm, ba, br, bs2_norm, bd)\n",
    "\n",
    "            # Update critics (twin)\n",
    "            loss1, loss2 = algo.update_critic(train_batch)\n",
    "            critic_loss = (loss1 + loss2) / 2.0\n",
    "\n",
    "            # Actor update schedule & warmup\n",
    "            if (t > algo.critic_warmup_steps) and (t % algo.actor_update_every == 0):\n",
    "                # pass normalized states\n",
    "                actor_loss, kl = algo.update_actor(bs_norm, K=16)\n",
    "            else:\n",
    "                actor_loss, kl = float('nan'), float('nan')\n",
    "\n",
    "            # Periodic status print\n",
    "            if t % 1000 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"[{t:6d}] Critic Loss1: {loss1:.4e} | Critic Loss2: {loss2:.4e} | \"\n",
    "                      f\"Actor Loss: {actor_loss:.4f} | KL: {kl:.4f} | Time: {elapsed:.1f}s\")\n",
    "\n",
    "        # Update reference policy\n",
    "        if t % ref_update_freq == 0 and t > 0:\n",
    "            algo.update_reference()\n",
    "            print(f\"[{t:6d}] Updated reference policy\")\n",
    "\n",
    "        # Evaluation\n",
    "        if t % eval_freq == 0 and t > 0 and (t - last_eval) >= eval_freq:\n",
    "            eval_reward = evaluate(env, algo, eval_episodes, act_low, act_high, obs_rms)\n",
    "            print(f\"[{t:6d}] ===== EVAL: {eval_reward:.2f} =====\")\n",
    "            last_eval = t\n",
    "\n",
    "    print(\"\\nTraining finished!\")\n",
    "    return algo\n",
    "\n",
    "def evaluate(env, algo, num_episodes, act_low, act_high, obs_rms):\n",
    "    \"\"\"Evaluate policy deterministically (uses obs normalization).\"\"\"\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        ts = env.reset()\n",
    "        s = flatten_obs(ts.observation)\n",
    "        ep_reward = 0.0\n",
    "        \n",
    "        while not ts.last():\n",
    "            s_norm = obs_rms.normalize(s)\n",
    "            a = algo.act(s_norm, deterministic=True)\n",
    "            a = np.clip(a, act_low, act_high)\n",
    "            ts = env.step(a)\n",
    "            s = flatten_obs(ts.observation)\n",
    "            r = ts.reward\n",
    "            if r is None:\n",
    "                r = 0.0\n",
    "            ep_reward += r\n",
    "        \n",
    "        total_reward += ep_reward\n",
    "    \n",
    "    return total_reward / num_episodes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_agent = train(\n",
    "        domain=\"cartpole\",\n",
    "        task=\"swingup\",\n",
    "        total_steps=500000,\n",
    "        batch_size=256,\n",
    "        start_steps=1000,\n",
    "        ref_update_freq=1000,\n",
    "        eval_freq=5000,\n",
    "        eval_episodes=5,\n",
    "        seed=0\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
